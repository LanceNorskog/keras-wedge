{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similarity mnist_convnet",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUri8sQeayhV"
      },
      "source": [
        "# Measure Feature Map Similarity\n",
        "This notebook is an enhanced version of a notebook in the Keras examples:\n",
        "[Simple MNIST convnet](https://keras.io/examples/vision/mnist_convnet/)\n",
        "\n",
        "Convolutional Neural Network (CNN) architectures use *feature maps* to capture aspects of an image. \n",
        "\n",
        "Since the set of feature maps is the complete inventory of features of an image found by a CNN, a well-trained model should not have redundant feature maps- the feature maps should all be different. This notebook introduces a measurement of similarity across feature maps with the aim of avoiding redundant feature maps.\n",
        "\n",
        "We will train a simple CNN against the standard MNIST stroke-digit dataset and will demonstrate how the mean similarity of feature maps slowly drops during training. We will also display feature map activations against an original MNIST image to illuminate how feature map similarity is a good measurement of the quality of a CNN model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CxhUq8Jw8HR"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2K6HeZJw8HS"
      },
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XblsUet2w8HT"
      },
      "source": [
        "## Build the model\n",
        "The model in the original notebook is broken out into two models:\n",
        "\n",
        "1.   a sub-model which emits the output of the CNN\n",
        "2.   a parent model for training purposes\n",
        "\n",
        "This allows us to extract feature maps and measure similarity in a callback function.\n",
        "\n",
        "Remember, a Model is also a Layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrGjv6aIw8HT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5cf1734-8c97-48fc-b643-e91de11d7916"
      },
      "source": [
        "cnn_model = keras.Sequential(\n",
        "    [\n",
        "        layers.InputLayer(input_shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "    ]\n",
        "    , name='CNN_sub_model'\n",
        ")\n",
        "cnn_model.summary()\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.InputLayer(input_shape=input_shape),\n",
        "        cnn_model,\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ],\n",
        "    name='Parent_model'\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"CNN_sub_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "=================================================================\n",
            "Total params: 18,816\n",
            "Trainable params: 18,816\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"Parent_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "CNN_sub_model (Sequential)   (None, 11, 11, 64)        18816     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYlVzLUb0Dbd"
      },
      "source": [
        "## Log image similarities during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q65Q1uYexczw"
      },
      "source": [
        "Add a Callback that fetches the final set of feature maps generated by the model, and calculates the average similarity of a random subset of pairs of feature maps.\n",
        "\n",
        "There are various ways to calculate similarity. This multiplies the two feature maps together and counts the resulting \"high\" valued cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2_0Cugtxau3"
      },
      "source": [
        "img_array = x_test[0:1]\n",
        "\n",
        "def similarity_multiply(img1, img2):\n",
        "    # norm both to 0->1, multiply to produce 0->1\n",
        "    min1 = np.min(img1)\n",
        "    min2 = np.min(img2)\n",
        "    base1 = np.max(img1) - min1\n",
        "    base2 = np.max(img2) - min2\n",
        "    if base1 == 0:\n",
        "        base1 = 0.0001\n",
        "    if base2 == 0:\n",
        "        base2 = 0.0001\n",
        "    norm1 = (img1 - min1) / base1\n",
        "    norm2 = (img2 - min2) / base2\n",
        "    mult = norm1 * norm2\n",
        "    correlated = mult > np.mean(mult)\n",
        "\n",
        "    percentage = sum(correlated.flatten()) / len(img1.flatten())\n",
        "    return percentage\n",
        "\n",
        "# While training, capture and log the mean similarity of the feature map pairs.\n",
        "# This network only has 64 fmaps, so it's ok to just check every pair.\n",
        "# This is using when training the complete network, but calls predict()\n",
        "# on the sub-network to fetch the feature maps.\n",
        "\n",
        "class LogSimilarities(keras.callbacks.Callback):\n",
        "    def __init__(self, cnn_model, img_array, simfunc):\n",
        "        super(LogSimilarities, self).__init__()\n",
        "        self._cnn_model = cnn_model\n",
        "        self._img_array = img_array\n",
        "        self._simfunc = simfunc\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        maps = self._cnn_model.predict(self._img_array)[:, :, :, :]\n",
        "        preds = []\n",
        "        for i in range(maps.shape[3]):\n",
        "            preds.append(maps[0, :, :, i])\n",
        "        sims = []\n",
        "        for i in range(maps.shape[3]):\n",
        "            for j in range(i + 1, maps.shape[3]):\n",
        "                measure = self._simfunc(preds[i], preds[j])\n",
        "                sims.append(measure)\n",
        "        avg = sum(sims)/len(sims)\n",
        "        if logs:\n",
        "            if 'similarity' not in logs:\n",
        "                logs['similarity'] = []\n",
        "            logs['similarity'].append(avg)\n",
        "        else:\n",
        "            print('Epoch: ' + epoch + ', mean similarity: ' + avg)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQcGzDZYw8HU"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_wAyLvmw8HU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c8c644-69ac-4e0b-c749-f4b6869ebf25"
      },
      "source": [
        "batch_size = 512\n",
        "epochs = 30\n",
        "\n",
        "simfunc = similarity_multiply\n",
        "logsim = LogSimilarities(cnn_model, img_array, simfunc=simfunc)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1,\n",
        "          callbacks=[logsim])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "106/106 [==============================] - 3s 8ms/step - loss: 0.7420 - accuracy: 0.7794 - val_loss: 0.1529 - val_accuracy: 0.9607\n",
            "Epoch 2/30\n",
            "106/106 [==============================] - 1s 5ms/step - loss: 0.1904 - accuracy: 0.9431 - val_loss: 0.0961 - val_accuracy: 0.9738\n",
            "Epoch 3/30\n",
            "106/106 [==============================] - 1s 5ms/step - loss: 0.1375 - accuracy: 0.9583 - val_loss: 0.0752 - val_accuracy: 0.9810\n",
            "Epoch 4/30\n",
            "106/106 [==============================] - 1s 6ms/step - loss: 0.1104 - accuracy: 0.9673 - val_loss: 0.0626 - val_accuracy: 0.9828\n",
            "Epoch 5/30\n",
            "106/106 [==============================] - 1s 6ms/step - loss: 0.0943 - accuracy: 0.9712 - val_loss: 0.0590 - val_accuracy: 0.9837\n",
            "Epoch 6/30\n",
            "106/106 [==============================] - 1s 6ms/step - loss: 0.0852 - accuracy: 0.9740 - val_loss: 0.0515 - val_accuracy: 0.9860\n",
            "Epoch 7/30\n",
            " 64/106 [=================>............] - ETA: 0s - loss: 0.0759 - accuracy: 0.9771"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns8j6TDtMQb1"
      },
      "source": [
        "## Analyze Similarity of Feature Maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps4Kr0tOx7j4"
      },
      "source": [
        "!pip install --force-reinstall -qq git+https://github.com/LanceNorskog/keract.git\n",
        "import keract    \n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSRxW1HhzZZd"
      },
      "source": [
        "### Plot mean similarity over epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM8gM_dGLZcm"
      },
      "source": [
        "Let's plot the **similarity** value gathered during training. This is the mean similarity between all pairs of the 64 feature maps generated by the CNN network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWf1sGxnzflN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_similarity_stats(history):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
        "    ax.plot(history['similarity'], label='similarity')\n",
        "    ax.plot(history['val_loss'], label='val_loss')\n",
        "    legend = ax.legend(loc='upper center', shadow=True, fontsize='large')\n",
        "    ax.set(xlabel='epochs', title='')\n",
        "\n",
        "plot_similarity_stats(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTKqC3tSseGS"
      },
      "source": [
        "This chart demonstrates how the drop in similarity tracks the improvement of the CNN (val_loss). CNN feature maps will slowly become decorrelated during a stable training cycle. Also notice how the similarity continues to drop as the network overtrains (val_loss starts increasing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEpYkpw7w8HU"
      },
      "source": [
        "## Visualize the Feature Maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irTsrrM__du3"
      },
      "source": [
        "def plot_heatmaps(img_array, fmap_i, fmap_j, similarity=None):\n",
        "    sim_label = ''\n",
        "    if similarity:\n",
        "        sim_label = \"{:.2f}\".format(similarity)\n",
        "    feature_maps = np.zeros((1, fmap_i.shape[0], fmap_i.shape[1], 3), dtype='float32')\n",
        "    feature_maps[0, :, :, 0] = fmap_i\n",
        "    feature_maps[0, :, :, 1] = fmap_j\n",
        "    feature_maps[0, :, :, 2] = fmap_i[:,:] * fmap_j[:,:]\n",
        "    activations = {sim_label: feature_maps}\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 12))\n",
        "    keract.display_heatmaps_1(activations, img_array, in_fig=fig, in_axes=axes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ymCZC_exH2E"
      },
      "source": [
        "Calculate and display similarity over all pairs of feature maps. Keep the image pair with the maximum and minimum similarity.\n",
        "\n",
        "Phillipe Remy's \"Keract\" library provides a very handy toolkit for fetching all of the feature maps generate for an image. It also will adorn the original image with data from a feature map to create a \"heatmap\", which superimposes the feature map onto the original image used to make the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvVQGeaLqsx-"
      },
      "source": [
        "maps = cnn_model.predict(img_array)[:, :, :, :]\n",
        "preds = []\n",
        "for i in range(maps.shape[3]):\n",
        "    preds.append(maps[0, :, :, i])\n",
        "preds = np.asarray(preds)\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(preds.reshape(-1, 1))\n",
        "\n",
        "\n",
        "sims = []\n",
        "x = 0\n",
        "min_i = -1\n",
        "min_j = -1\n",
        "max_i = -1\n",
        "max_j = -1\n",
        "min_sim = 100000\n",
        "max_sim = -1\n",
        "\n",
        "for i in range(len(preds)):\n",
        "    for j in range(i + 1, len(preds)):\n",
        "        measure = simfunc(preds[i], preds[j])\n",
        "        top_i = np.max(preds[i])\n",
        "        top_j = np.max(preds[j])\n",
        "        ratio = np.max([top_i, top_j])/np.min([top_i, top_j])\n",
        "        if measure < min_sim and ratio < 3:\n",
        "            min_sim = measure\n",
        "            min_i = i\n",
        "            min_j = j\n",
        "        if measure > max_sim:\n",
        "            max_sim = measure\n",
        "            max_i = i\n",
        "            max_j = j\n",
        "        sims.append(measure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YADu7Ipf7QB"
      },
      "source": [
        "activations = {'': maps}\n",
        "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
        "axes[3][2].grid(color='r', linestyle='-', linewidth=2)\n",
        "keract.display_heatmaps_1(activations, img_array, in_fig=fig, in_axes=axes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBTsiiC1kR3X"
      },
      "source": [
        "These 64 heatmaps are \"features\" or \"aspects\" of what the CNN notices about the handwritten digit '7'. There are several different measurements of horizontal and diagonal strokes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A7ZfPh1xODQ"
      },
      "source": [
        "Note:\n",
        "> These images are a great demonstration of the \"translation invariance\" property of Convolutional Neural Networks. As the image is processed by a stack of Conv2D layers, activations \"slide across\" the image. Different input images with the same features in different places in the image can activate the same feature map. This is why a feature map might \"light up\" next to the handwritten stroke rather than on it: the handwritten digits are all roughly the same size, but they are placed differently inside the image. The feature maps pick an \"average\" placement for a horizontal or diagonal stroke.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7UcKnfxjNd7"
      },
      "source": [
        "### Similar Feature Maps\n",
        "Next we will display the most similar pair of feature maps above, and then multiply the two feature maps together in Hadamard (cell-wise) mode to demonstrate their correlation. The left and middle images are the two feature maps, the rightmost image is the two feature maps multiplied together. This is the core idea of the similarity measure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7su9leeBa85"
      },
      "source": [
        "plot_heatmaps(img_array, preds[max_i], preds[max_j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt5AUYHmj3Zt"
      },
      "source": [
        "Here we do the same with the least similar feature maps. Since they have no common areas, there no activated areas on the right. \n",
        "\n",
        "The rightmost image has a darker background because high and low activations are exaggerated by multiplying."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSDMdQLUB0Oi"
      },
      "source": [
        "plot_heatmaps(img_array, preds[min_i], preds[min_j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1arAMscOMge"
      },
      "source": [
        "## Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfN03FLH7IW7"
      },
      "source": [
        "It is clear from this demonstration that feature map similarity is a useful measurement of the quality of a convolutional neural network: as the network improves, the mean similarity will drop. \n",
        "\n",
        "The reason for this is simple: good feature maps are decorrelated. Feature maps are *independent captures* of features (parts of images) that happen over and over in the input images. If multiple feature maps describe the same feature, then processing power is being wasted. The **descriptive bandwidth** of the feature maps is optimized when no two feature maps describe the same feature.\n",
        "\n",
        "Based on this insight, it should be possible to improve a CNN by measuring feature map similarity and providing feedback via the loss function. This is the core idea behind Wedge Dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46p6I5PgcX_u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}