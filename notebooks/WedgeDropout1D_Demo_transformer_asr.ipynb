{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WedgeDropout1D Demo transformer_asr",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0REZh2eVxS5W"
      },
      "source": [
        "# Automatic Speech Recognition with Transformer\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2021/01/13<br>\n",
        "**Last modified:** 2021/01/13<br>\n",
        "**Description:** Training a sequence-to-sequence Transformer for automatic speech recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-gi25vixS5e"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n",
        "ASR can be treated as a sequence-to-sequence problem, where the\n",
        "audio can be represented as a sequence of feature vectors\n",
        "and the text as a sequence of characters, words, or subword tokens.\n",
        "\n",
        "For this demonstration, we will use the LJSpeech dataset from the\n",
        "[LibriVox](https://librivox.org/) project. It consists of short\n",
        "audio clips of a single speaker reading passages from 7 non-fiction books.\n",
        "Our model will be similar to the original Transformer (both encoder and decoder)\n",
        "as proposed in the paper, \"Attention is All You Need\".\n",
        "\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/pdf/1904.13377.pdf)\n",
        "- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n",
        "- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKldHskZxS5f"
      },
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZOpS1UuxS5f"
      },
      "source": [
        "## Define the Transformer Input Layer\n",
        "\n",
        "When processing past target tokens for the decoder, we compute the sum of\n",
        "position embeddings and token embeddings.\n",
        "\n",
        "When processing audio features, we apply convolutional layers to downsample\n",
        "them (via convolution stides) and process local relationships."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ApevpBjxS5g"
      },
      "source": [
        "\n",
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
        "        super().__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "\n",
        "\n",
        "class SpeechFeatureEmbedding(layers.Layer):\n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.conv3(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7USR0gRjxS5g"
      },
      "source": [
        "## Transformer Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3PTXmOhxS5g"
      },
      "source": [
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sihr4wwtxS5h"
      },
      "source": [
        "## Transformer Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4eVxRlnxS5h"
      },
      "source": [
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.self_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.self_dropout = layers.Dropout(0.5)\n",
        "        self.enc_dropout = layers.Dropout(0.1)\n",
        "        self.ffn_dropout = layers.Dropout(0.1)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
        "\n",
        "        This prevents flow of information from future tokens to current token.\n",
        "        1's in the lower triangle, counting from the lower right corner.\n",
        "        \"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_out, target):\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
        "        ffn_out = self.ffn(enc_out_norm)\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
        "        return ffn_out_norm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulPvv8dPxS5i"
      },
      "source": [
        "## Complete the Transformer model\n",
        "\n",
        "Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n",
        "During training, we give the decoder the target character sequence shifted to the left\n",
        "as input. During inference, the decoder uses its own past predictions to predict the\n",
        "next token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYLzvbq_xS5i"
      },
      "source": [
        "\n",
        "class Transformer(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=64,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        source_maxlen=100,\n",
        "        target_maxlen=100,\n",
        "        num_layers_enc=4,\n",
        "        num_layers_dec=1,\n",
        "        num_classes=10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
        "        )\n",
        "\n",
        "        self.encoder = keras.Sequential(\n",
        "            [self.enc_input]\n",
        "            + [\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "                for _ in range(num_layers_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(\n",
        "                self,\n",
        "                f\"dec_layer_{i}\",\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
        "            )\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "    def decode(self, enc_out, target):\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs):\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source)\n",
        "        y = self.decode(x, target)\n",
        "        return self.classifier(y)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "        return dec_input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ4IP8rhxS5j"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "Note: This requires ~3.6 GB of disk space and\n",
        "takes ~5 minutes for the extraction of files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5r2MRAkxS5k"
      },
      "source": [
        "if not os.path.exists('data/tar/gz'):\n",
        "    keras.utils.get_file(\n",
        "        os.path.join(os.getcwd(), \"data.tar.gz\"),\n",
        "        \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n",
        "        extract=True,\n",
        "        archive_format=\"tar\",\n",
        "        cache_dir=\".\",\n",
        "    )\n",
        "\n",
        "\n",
        "    saveto = \"./datasets/LJSpeech-1.1\"\n",
        "    wavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n",
        "\n",
        "    id_to_text = {}\n",
        "    with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            id = line.strip().split(\"|\")[0]\n",
        "            text = line.strip().split(\"|\")[2]\n",
        "            id_to_text[id] = text\n",
        "\n",
        "\n",
        "def get_data(wavs, id_to_text, maxlen=50):\n",
        "    \"\"\" returns mapping of audio paths and transcription texts \"\"\"\n",
        "    data = []\n",
        "    for w in wavs:\n",
        "        id = w.split(\"/\")[-1].split(\".\")[0]\n",
        "        if len(id_to_text[id]) < maxlen:\n",
        "            data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
        "    return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dF2-mUGxS5k"
      },
      "source": [
        "## Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDAuMT1DxS5k",
        "outputId": "40daa15f-2548-4afa-a87b-c8fc2807ae52"
      },
      "source": [
        "\n",
        "class VectorizeChar:\n",
        "    def __init__(self, max_len=50):\n",
        "        self.vocab = (\n",
        "            [\"-\", \"#\", \"<\", \">\"]\n",
        "            + [chr(i + 96) for i in range(1, 27)]\n",
        "            + [\" \", \".\", \",\", \"?\"]\n",
        "        )\n",
        "        self.max_len = max_len\n",
        "        self.char_to_idx = {}\n",
        "        for i, ch in enumerate(self.vocab):\n",
        "            self.char_to_idx[ch] = i\n",
        "\n",
        "    def __call__(self, text):\n",
        "        text = text.lower()\n",
        "        text = text[: self.max_len - 2]\n",
        "        text = \"<\" + text + \">\"\n",
        "        pad_len = self.max_len - len(text)\n",
        "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        return self.vocab\n",
        "\n",
        "\n",
        "max_target_len = 200  # all transcripts in out data are < 200 characters\n",
        "data = get_data(wavs, id_to_text, max_target_len)\n",
        "vectorizer = VectorizeChar(max_target_len)\n",
        "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
        "\n",
        "\n",
        "def create_text_ds(data):\n",
        "    texts = [_[\"text\"] for _ in data]\n",
        "    text_ds = [vectorizer(t) for t in texts]\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
        "    return text_ds\n",
        "\n",
        "\n",
        "def path_to_audio(path):\n",
        "    # spectrogram using stft\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
        "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
        "    # normalisation\n",
        "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
        "    x = (x - means) / stddevs\n",
        "    audio_len = tf.shape(x)[0]\n",
        "    # padding to 10 seconds\n",
        "    pad_len = 2754\n",
        "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
        "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "def create_audio_ds(data):\n",
        "    flist = [_[\"audio\"] for _ in data]\n",
        "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
        "    audio_ds = audio_ds.map(\n",
        "        path_to_audio, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "    )\n",
        "    return audio_ds\n",
        "\n",
        "\n",
        "def create_tf_dataset(data, bs=4):\n",
        "    audio_ds = create_audio_ds(data)\n",
        "    text_ds = create_text_ds(data)\n",
        "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
        "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
        "    ds = ds.batch(bs)\n",
        "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "\n",
        "split = int(len(data) * 0.99)\n",
        "train_data = data[:split]\n",
        "test_data = data[split:]\n",
        "ds = create_tf_dataset(train_data, bs=256)\n",
        "val_ds = create_tf_dataset(test_data, bs=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKN5wzQhxS5l"
      },
      "source": [
        "## Callbacks to display predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y_b9g2MxS5l"
      },
      "source": [
        "\n",
        "class DisplayOutputs(keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
        "    ):\n",
        "        \"\"\"Displays a batch of outputs after every epoch\n",
        "\n",
        "        Args:\n",
        "            batch: A test batch containing the keys \"source\" and \"target\"\n",
        "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
        "            target_start_token_idx: A start token index in the target vocabulary\n",
        "            target_end_token_idx: An end token index in the target vocabulary\n",
        "        \"\"\"\n",
        "        self.batch = batch\n",
        "        self.target_start_token_idx = target_start_token_idx\n",
        "        self.target_end_token_idx = target_end_token_idx\n",
        "        self.idx_to_char = idx_to_token\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 5 != 0:\n",
        "            return\n",
        "        source = self.batch[\"source\"]\n",
        "        target = self.batch[\"target\"].numpy()\n",
        "        bs = tf.shape(source)[0]\n",
        "        preds = self.model.generate(source, self.target_start_token_idx)\n",
        "        preds = preds.numpy()\n",
        "        for i in range(bs):\n",
        "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
        "            prediction = \"\"\n",
        "            for idx in preds[i, :]:\n",
        "                prediction += self.idx_to_char[idx]\n",
        "                if idx == self.target_end_token_idx:\n",
        "                    break\n",
        "            print(f\"target:     {target_text.replace('-','')}\")\n",
        "            print(f\"prediction: {prediction}\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YW2xIm6xS5l"
      },
      "source": [
        "## Learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP2lZGyTxS5l"
      },
      "source": [
        "\n",
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        init_lr=0.00001,\n",
        "        lr_after_warmup=0.001,\n",
        "        final_lr=0.00001,\n",
        "        warmup_epochs=15,\n",
        "        decay_epochs=85,\n",
        "        steps_per_epoch=203,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.init_lr = init_lr\n",
        "        self.lr_after_warmup = lr_after_warmup\n",
        "        self.final_lr = final_lr\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.decay_epochs = decay_epochs\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def calculate_lr(self, epoch):\n",
        "        \"\"\" linear warm up - linear decay \"\"\"\n",
        "        warmup_lr = (\n",
        "            self.init_lr\n",
        "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
        "        )\n",
        "        decay_lr = tf.math.maximum(\n",
        "            self.final_lr,\n",
        "            self.lr_after_warmup\n",
        "            - (epoch - self.warmup_epochs)\n",
        "            * (self.lr_after_warmup - self.final_lr)\n",
        "            / (self.decay_epochs),\n",
        "        )\n",
        "        return tf.math.minimum(warmup_lr, decay_lr)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        epoch = step // self.steps_per_epoch\n",
        "        return self.calculate_lr(epoch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZOGRnVkxS5m"
      },
      "source": [
        "## Create & train the end-to-end model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opKqs6fQxS5m",
        "outputId": "fddbc66a-8b01-4c63-d519-cafac82dc7d6"
      },
      "source": [
        "batch = next(iter(val_ds))\n",
        "\n",
        "# The vocabulary to convert predicted indices into characters\n",
        "idx_to_char = vectorizer.get_vocabulary()\n",
        "display_cb = DisplayOutputs(\n",
        "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",
        ")  # set the arguments as per vocabulary index for '<' and '>'\n",
        "\n",
        "model = Transformer(\n",
        "    num_hid=200,\n",
        "    num_head=2,\n",
        "    num_feed_forward=400,\n",
        "    target_maxlen=max_target_len,\n",
        "    num_layers_enc=4,\n",
        "    num_layers_dec=1,\n",
        "    num_classes=34,\n",
        ")\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, label_smoothing=0.1,\n",
        ")\n",
        "\n",
        "learning_rate = CustomSchedule(\n",
        "    init_lr=0.00001,\n",
        "    lr_after_warmup=0.001,\n",
        "    final_lr=0.00001,\n",
        "    warmup_epochs=15,\n",
        "    decay_epochs=85,\n",
        "    steps_per_epoch=len(ds),\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "epochs=100\n",
        "\n",
        "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb, earlystopping], epochs=epochs, verbose=2)\n",
        "print(model.evaluate(val_ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "51/51 - 204s - loss: 1.9901 - val_loss: 1.9623\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <nneannnigeioett .es qtan t mth mo m taet oetan oer t t mi oneqr mf s cn.cnnre>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <nneannnigeioett .es qtan t mth mo m taet oetan oer t t mi oneqr mf s cn.cnnre>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <nneannnigeioett .es qtan t mth mo m taet oetan oer t t mi oneqr mf s cn.cnnre>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <nneannnigeioett .es qtan t mth mo m taet oetan oer t t mi oneqr mf s cn.cnnre>\n",
            "\n",
            "Epoch 2/100\n",
            "51/51 - 194s - loss: 1.5454 - val_loss: 1.5374\n",
            "Epoch 3/100\n",
            "51/51 - 193s - loss: 1.3885 - val_loss: 1.4528\n",
            "Epoch 4/100\n",
            "51/51 - 194s - loss: 1.3398 - val_loss: 1.4232\n",
            "Epoch 5/100\n",
            "51/51 - 191s - loss: 1.3199 - val_loss: 1.4106\n",
            "Epoch 6/100\n",
            "51/51 - 191s - loss: 1.3102 - val_loss: 1.4041\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <the as the athe the the the an athe the the are the athe the the the the are the the the the the the the are the the therere.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <the athe the the the the athe the the the the are the the the athe the the are the an the an the the the are the the the the the the the the the ane therere ore of thenthent of nedy.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <the as the athe the the the an athe the the an the the the an athe the the the the the the athe are the the the therere.>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the athe the the the the the the the the are the the the the are the the are the ore the the thererererere.>\n",
            "\n",
            "Epoch 7/100\n",
            "51/51 - 191s - loss: 1.3053 - val_loss: 1.3995\n",
            "Epoch 8/100\n",
            "51/51 - 191s - loss: 1.3012 - val_loss: 1.3959\n",
            "Epoch 9/100\n",
            "51/51 - 191s - loss: 1.2977 - val_loss: 1.3914\n",
            "Epoch 10/100\n",
            "51/51 - 190s - loss: 1.2905 - val_loss: 1.3801\n",
            "Epoch 11/100\n",
            "51/51 - 193s - loss: 1.2771 - val_loss: 1.3659\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <on the se the the the the the the the the the s the and the the the the the the the the the the the the the the the the the the the the the the the.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he are the and the and athe an the the and and the the the and and an the the are the the and an and the ane and the the the the the the and the ane ane the the ofothenthenthed nedy.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <he the the the the the the the the the ware the the the the the the the the the the wan the the the the the the the the the the.>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the the the the ware the the the the the are the the the arofore wan the an the the wan the the there there the.>\n",
            "\n",
            "Epoch 12/100\n",
            "51/51 - 189s - loss: 1.2621 - val_loss: 1.3497\n",
            "Epoch 13/100\n",
            "51/51 - 189s - loss: 1.2390 - val_loss: 1.3169\n",
            "Epoch 14/100\n",
            "51/51 - 190s - loss: 1.2119 - val_loss: 1.2801\n",
            "Epoch 15/100\n",
            "51/51 - 189s - loss: 1.1570 - val_loss: 1.2105\n",
            "Epoch 16/100\n",
            "51/51 - 189s - loss: 1.0615 - val_loss: 1.0955\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the sament same and the same and and samand and the same and the same and mand and sand.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he arde the and the ad pearing the plaring the and the plary and the plaring the plary and the peary pland the plareareareareatheatheatheat.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <mean with dis patering the and with a reand with and with and and with and the reand with and with a lon the and thererere>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the manosed anoter the and noter the and nof ther the anof the prone.>\n",
            "\n",
            "Epoch 17/100\n",
            "51/51 - 189s - loss: 0.9639 - val_loss: 1.0038\n",
            "Epoch 18/100\n",
            "51/51 - 190s - loss: 0.8772 - val_loss: 0.9354\n",
            "Epoch 19/100\n",
            "51/51 - 189s - loss: 0.8186 - val_loss: 0.8913\n",
            "Epoch 20/100\n",
            "51/51 - 193s - loss: 0.7767 - val_loss: 0.8654\n",
            "Epoch 21/100\n",
            "51/51 - 192s - loss: 0.7478 - val_loss: 0.8326\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the paintic supporters and supporters and mand and supporters and mand, one of the patroy at fic pat.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had bery finding a difficulty apear do have bery fulty apeary fulty apeary fulty apeared peary fulty apeared pear he he heave he ave ave he ave ad ave avere plded,>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <mean with discovery puspatteriough he with discovery puspatteriou the dispatteriok the he unif spateriough he mring s>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of hot her hot har hot har hot her hot her hot her hot her hot for no sever hot for no sever hot for no.>\n",
            "\n",
            "Epoch 22/100\n",
            "51/51 - 193s - loss: 0.7200 - val_loss: 0.8165\n",
            "Epoch 23/100\n",
            "51/51 - 193s - loss: 0.6970 - val_loss: 0.8052\n",
            "Epoch 24/100\n",
            "51/51 - 189s - loss: 0.6778 - val_loss: 0.7928\n",
            "Epoch 25/100\n",
            "51/51 - 190s - loss: 0.6625 - val_loss: 0.7894\n",
            "Epoch 26/100\n",
            "51/51 - 189s - loss: 0.6505 - val_loss: 0.7848\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the sament sight and sires and sight and sires and man agers and man agers and man agers and man agers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very finding a meany the appeared an emeaning finding a the appeared and the appeared an emeany the appeared fininifichererered thered cory cory counin thed.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <mean with he discovery ou hould the discovery of spatered with he discovery of the discovery of the dis>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better they what for no senpith burt for no senpithy whet for no senpithy whet for no senpiter they whe.>\n",
            "\n",
            "Epoch 27/100\n",
            "51/51 - 189s - loss: 0.6380 - val_loss: 0.7758\n",
            "Epoch 28/100\n",
            "51/51 - 189s - loss: 0.6285 - val_loss: 0.7671\n",
            "Epoch 29/100\n",
            "51/51 - 189s - loss: 0.6184 - val_loss: 0.7615\n",
            "Epoch 30/100\n",
            "51/51 - 189s - loss: 0.6064 - val_loss: 0.7539\n",
            "Epoch 31/100\n",
            "51/51 - 189s - loss: 0.5968 - val_loss: 0.7493\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the saman agers and managers and managers and managers and managers and managers and managers and managers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very do have greats were do have greats werelaty any a meany a meany the advery feared any a meany the appeared athe wore arlthe are the ce arld.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <mean willod and brit pestoland, and brith the dis covery of and brilod and brilods>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of arth, could hot aver his crime. crime. crime.>\n",
            "\n",
            "Epoch 32/100\n",
            "51/51 - 188s - loss: 0.5886 - val_loss: 0.7528\n",
            "Epoch 33/100\n",
            "51/51 - 187s - loss: 0.5822 - val_loss: 0.7511\n",
            "Epoch 34/100\n",
            "51/51 - 187s - loss: 0.5765 - val_loss: 0.7531\n",
            "Epoch 35/100\n",
            "51/51 - 187s - loss: 0.5702 - val_loss: 0.7512\n",
            "Epoch 36/100\n",
            "51/51 - 187s - loss: 0.5636 - val_loss: 0.7442\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the samannagers and managers and and supporters and managers and supporters and managers and managers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very finding a meany, close withought any close withought any close withoughty appeared any a peared other pearel great grltlthed.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <mean wild the discovery of a pisspatoral, and blood the dis covery of a piscovery of a pisse>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of burt for no seman of burt for no sempir no seme.>\n",
            "\n",
            "Epoch 37/100\n",
            "51/51 - 187s - loss: 0.5572 - val_loss: 0.7396\n",
            "Epoch 38/100\n",
            "51/51 - 187s - loss: 0.5507 - val_loss: 0.7413\n",
            "Epoch 39/100\n",
            "51/51 - 187s - loss: 0.5453 - val_loss: 0.7454\n",
            "Epoch 40/100\n",
            "51/51 - 187s - loss: 0.5408 - val_loss: 0.7468\n",
            "Epoch 41/100\n",
            "51/51 - 187s - loss: 0.5374 - val_loss: 0.7467\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the saman agers and managers and man agers and man agers and man agers and man agers and man agers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had vary fing a mean eadvary finding a peoplace appeared oher do have great apppeared other peoplace appeared other pl peoplopl ced.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <mean breet and brilod the discovery of the discol and brielod and brilod the dis.>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of birt they what hoke at hoke at hould for no sempir no sempirt ther but for no sem.>\n",
            "\n",
            "Epoch 42/100\n",
            "51/51 - 187s - loss: 0.5319 - val_loss: 0.7455\n",
            "Epoch 43/100\n",
            "51/51 - 188s - loss: 0.5242 - val_loss: 0.7494\n",
            "Epoch 44/100\n",
            "51/51 - 187s - loss: 0.5200 - val_loss: 0.7425\n",
            "Epoch 45/100\n",
            "51/51 - 186s - loss: 0.5176 - val_loss: 0.7466\n",
            "Epoch 46/100\n",
            "51/51 - 187s - loss: 0.5164 - val_loss: 0.7508\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the saman and sidetrs and managers and managers and managers and managers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had vary finding a meany, close werelty it appeared other peoplace withough it any, close withought other peoplace withough inghed.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <mean brit piscovery of pistol and blood the discovery of pistal and blood the dispattered with heril and blood the>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of birt ever his crime. crime.>\n",
            "\n",
            "Epoch 47/100\n",
            "51/51 - 187s - loss: 0.5127 - val_loss: 0.7458\n",
            "33/33 [==============================] - 2s 65ms/step - loss: 0.7396\n",
            "0.7396112084388733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQLX44PxxS5m"
      },
      "source": [
        "In practice, you should train for around 100 epochs or more.\n",
        "\n",
        "Some of the predicted text at or around epoch 35 may look as follows:\n",
        "```\n",
        "target:     <as they sat in the car, frazier asked oswald where his lunch was>\n",
        "prediction: <as they sat in the car frazier his lunch ware mis lunch was>\n",
        "\n",
        "target:     <under the entry for may one, nineteen sixty,>\n",
        "prediction: <under the introus for may monee, nin the sixty,>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zXmsmrbyEH-"
      },
      "source": [
        "# WedgeDropout1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2gGzyzVbyInf",
        "outputId": "970c7a15-e988-4d01-f083-efe08d92870f"
      },
      "source": [
        "!pip uninstall -y keras-wedge-dropout\n",
        "!pip install -q git+https://github.com/LanceNorskog/keras-wedge.git\n",
        "from keras_wedge_dropout import WedgeDropout1D"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: keras-wedge-dropout 0.1.0\n",
            "Uninstalling keras-wedge-dropout-0.1.0:\n",
            "  Successfully uninstalled keras-wedge-dropout-0.1.0\n",
            "  Building wheel for keras-wedge-dropout (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBENfj5vyUNi"
      },
      "source": [
        "\n",
        "class SpeechFeatureEmbeddingWedgeDropout(layers.Layer):\n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.wedge1d = WedgeDropout1D(similarity=0.65)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.wedge1d(x)\n",
        "        return self.conv3(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGb9SXSDyx9A"
      },
      "source": [
        "\n",
        "class TransformerWedgeDropout(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=64,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        source_maxlen=100,\n",
        "        target_maxlen=100,\n",
        "        num_layers_enc=4,\n",
        "        num_layers_dec=1,\n",
        "        num_classes=10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.enc_input = SpeechFeatureEmbeddingWedgeDropout(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
        "        )\n",
        "\n",
        "        self.encoder = keras.Sequential(\n",
        "            [self.enc_input]\n",
        "            + [\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "                for _ in range(num_layers_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(\n",
        "                self,\n",
        "                f\"dec_layer_{i}\",\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
        "            )\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "    def decode(self, enc_out, target):\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs):\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source)\n",
        "        y = self.decode(x, target)\n",
        "        return self.classifier(y)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "        return dec_input\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDnkPZxLzoPC",
        "outputId": "67fe0bb7-3df8-4e70-9622-01fb43f406b9"
      },
      "source": [
        "batch = next(iter(val_ds))\n",
        "\n",
        "# The vocabulary to convert predicted indices into characters\n",
        "idx_to_char = vectorizer.get_vocabulary()\n",
        "display_cb = DisplayOutputs(\n",
        "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",
        ")  # set the arguments as per vocabulary index for '<' and '>'\n",
        "\n",
        "model = TransformerWedgeDropout(\n",
        "    num_hid=200,\n",
        "    num_head=2,\n",
        "    num_feed_forward=400,\n",
        "    target_maxlen=max_target_len,\n",
        "    num_layers_enc=4,\n",
        "    num_layers_dec=1,\n",
        "    num_classes=34,\n",
        ")\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, label_smoothing=0.1,\n",
        ")\n",
        "\n",
        "learning_rate = CustomSchedule(\n",
        "    init_lr=0.00001,\n",
        "    lr_after_warmup=0.001,\n",
        "    final_lr=0.00001,\n",
        "    warmup_epochs=15,\n",
        "    decay_epochs=85,\n",
        "    steps_per_epoch=len(ds),\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "epochs=100\n",
        "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb, earlystopping], epochs=epochs, verbose=2)\n",
        "print(model.evaluate(val_ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "51/51 - 194s - loss: 1.9167 - val_loss: 1.9130\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: < ae v>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: < ae v>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: < ae v>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: < ae v>\n",
            "\n",
            "Epoch 2/100\n",
            "51/51 - 188s - loss: 1.5274 - val_loss: 1.5366\n",
            "Epoch 3/100\n",
            "51/51 - 188s - loss: 1.3841 - val_loss: 1.4537\n",
            "Epoch 4/100\n",
            "51/51 - 188s - loss: 1.3376 - val_loss: 1.4221\n",
            "Epoch 5/100\n",
            "51/51 - 188s - loss: 1.3178 - val_loss: 1.4091\n",
            "Epoch 6/100\n",
            "51/51 - 190s - loss: 1.3092 - val_loss: 1.4033\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <the as the as are the the the the the the the the the are the an the are the the the the the the the are the the the the the.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <the an the are the the the the an the the are the the an an an the an the the the the the an the the are the the the the the the there there there and anerererererererereren.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <the as the as are the the the the the the the the the as an an the are the the the the the the therere the there.>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the as the are the the the the the the the are the are the the the are the there therere therere.>\n",
            "\n",
            "Epoch 7/100\n",
            "51/51 - 190s - loss: 1.3044 - val_loss: 1.4011\n",
            "Epoch 8/100\n",
            "51/51 - 191s - loss: 1.3010 - val_loss: 1.3962\n",
            "Epoch 9/100\n",
            "51/51 - 191s - loss: 1.2971 - val_loss: 1.3904\n",
            "Epoch 10/100\n",
            "51/51 - 189s - loss: 1.2921 - val_loss: 1.3849\n",
            "Epoch 11/100\n",
            "51/51 - 190s - loss: 1.2840 - val_loss: 1.3720\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <on the the the the the the s are the the as ande the the the the fofofof the the the the the the the the the the the the the the f the sofoforere t.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he walld the the the are an the are the the an anofore the the the the the the the the the the the the the the are are the the the the the the the are are are the the theren kennedy.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <the the as the the the the the the are the an the the the the are the the the the the the the the the the the the the are thes.>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the the the fore the the fore the the the f the there the the the fore the therore f the the fore the ther.>\n",
            "\n",
            "Epoch 12/100\n",
            "51/51 - 189s - loss: 1.2663 - val_loss: 1.3521\n",
            "Epoch 13/100\n",
            "51/51 - 191s - loss: 1.2494 - val_loss: 1.3330\n",
            "Epoch 14/100\n",
            "51/51 - 191s - loss: 1.2208 - val_loss: 1.2924\n",
            "Epoch 15/100\n",
            "51/51 - 190s - loss: 1.1698 - val_loss: 1.2240\n",
            "Epoch 16/100\n",
            "51/51 - 189s - loss: 1.0844 - val_loss: 1.1287\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <a fine sand same and sand the same ssice of the pand sice of the same pand sand the same pand the same pat.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he and and and and the fire for the fear plaraing the and and and the pearar in the fifle the perear plararal ot the t in o in ot te te te te t t ote te te te te te.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <be and nispate of the the spatered and of the wenthen wen and and nin an nin nin the spand of the spates>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the mand nother of the and nother and of the and the mand of the and the peris prifor the cris pris prion.>\n",
            "\n",
            "Epoch 17/100\n",
            "51/51 - 189s - loss: 0.9897 - val_loss: 1.0315\n",
            "Epoch 18/100\n",
            "51/51 - 190s - loss: 0.8996 - val_loss: 0.9479\n",
            "Epoch 19/100\n",
            "51/51 - 190s - loss: 0.8301 - val_loss: 0.8922\n",
            "Epoch 20/100\n",
            "51/51 - 190s - loss: 0.7829 - val_loss: 0.8541\n",
            "Epoch 21/100\n",
            "51/51 - 190s - loss: 0.7491 - val_loss: 0.8330\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the pat fice ficfic portaicf fice ficessisisisisight ficfic ficfic ficfic fice ficessisisisisisight.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he have call peared the advery find the advele people people people peoplace and he advelty and he advelty any a and heand plllthe plthe pld pld.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <meing whit piscovery of the discovery of the discovery of piscovery of the discovery and with destol and nin with the dinens>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man and what arth for his crime what are hot for his crime.>\n",
            "\n",
            "Epoch 22/100\n",
            "51/51 - 189s - loss: 0.7224 - val_loss: 0.8202\n",
            "Epoch 23/100\n",
            "51/51 - 190s - loss: 0.6994 - val_loss: 0.8032\n",
            "Epoch 24/100\n",
            "51/51 - 189s - loss: 0.6798 - val_loss: 0.7902\n",
            "Epoch 25/100\n",
            "51/51 - 189s - loss: 0.6633 - val_loss: 0.7812\n",
            "Epoch 26/100\n",
            "51/51 - 189s - loss: 0.6498 - val_loss: 0.7716\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the supporters and supporters and supportary outicfic ficfic ficfic ficfic ficfic ficfic ficfic ficfic ficfic ficfic f fie f f f fien.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he advery find appeared other peared other peared other pear do have great in find a peoplace with appeared other peoplace athe athed ated of ted ared ated.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <mean warring whith piscovery appiscovery apiscovery of stold and blood the discovery appis,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better the for the for his crime what are hoper his crime.>\n",
            "\n",
            "Epoch 27/100\n",
            "51/51 - 189s - loss: 0.6393 - val_loss: 0.7698\n",
            "Epoch 28/100\n",
            "51/51 - 189s - loss: 0.6280 - val_loss: 0.7597\n",
            "Epoch 29/100\n",
            "51/51 - 188s - loss: 0.6171 - val_loss: 0.7561\n",
            "Epoch 30/100\n",
            "51/51 - 188s - loss: 0.6064 - val_loss: 0.7497\n",
            "Epoch 31/100\n",
            "51/51 - 187s - loss: 0.5969 - val_loss: 0.7484\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the supporters and supporters and supportic fiatry and supportic ficfic finety ngers and supportic ficfic ficfic finet.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very, in fould shrelationship a peopled appeared a peopled and he had very, in fould, and he had very, and he had co are anininininind.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <meanw brangrains fatter with pis fatter with pis fatter with pis fatter with piscovery of reight few the discovery of stols>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better better better better better better better better the for no sanppite.>\n",
            "\n",
            "Epoch 32/100\n",
            "51/51 - 187s - loss: 0.5882 - val_loss: 0.7442\n",
            "Epoch 33/100\n",
            "51/51 - 187s - loss: 0.5785 - val_loss: 0.7327\n",
            "Epoch 34/100\n",
            "51/51 - 187s - loss: 0.5685 - val_loss: 0.7273\n",
            "Epoch 35/100\n",
            "51/51 - 187s - loss: 0.5593 - val_loss: 0.7252\n",
            "Epoch 36/100\n",
            "51/51 - 187s - loss: 0.5511 - val_loss: 0.7188\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the samed of the supporters and managers and managers and managers and managers and managers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he edified a people, and he advery feew ith appeared other peared ohave greate few ithough it any few ithough it applace wit acothind.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <beinwal, and brains, and blood, and blood, and blood, and blood, and blood, and blod, and blood, and blood,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man ose crime nos crime nose crime.>\n",
            "\n",
            "Epoch 37/100\n",
            "51/51 - 186s - loss: 0.5441 - val_loss: 0.7077\n",
            "Epoch 38/100\n",
            "51/51 - 187s - loss: 0.5349 - val_loss: 0.7076\n",
            "Epoch 39/100\n",
            "51/51 - 187s - loss: 0.5287 - val_loss: 0.7046\n",
            "Epoch 40/100\n",
            "51/51 - 189s - loss: 0.5234 - val_loss: 0.7077\n",
            "Epoch 41/100\n",
            "51/51 - 189s - loss: 0.5184 - val_loss: 0.7048\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the same angers and managers and managers and managers and managers and managers and managers and managers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very few ith a people, in fonding ameaning ameaning fonding a meaning foral place relations in the worlate close in thed athe are ald.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <beinwal, the discovery a pistol and blood, the discovery afstel and blood, the discovery allod, and blood,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better burg the for his grime.>\n",
            "\n",
            "Epoch 42/100\n",
            "51/51 - 188s - loss: 0.5138 - val_loss: 0.7049\n",
            "Epoch 43/100\n",
            "51/51 - 187s - loss: 0.5088 - val_loss: 0.6971\n",
            "Epoch 44/100\n",
            "51/51 - 187s - loss: 0.5030 - val_loss: 0.6927\n",
            "Epoch 45/100\n",
            "51/51 - 186s - loss: 0.4976 - val_loss: 0.6953\n",
            "Epoch 46/100\n",
            "51/51 - 186s - loss: 0.4936 - val_loss: 0.6943\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the same anagers and managers and managers and managers and managers and managers and managers and managers and magers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very fucolse with any a people, and he had very fuew ithough it any, close with any ameaning fol place in folked and he grealdis areald at,>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <been with pis ftole and night and blood, and night and blood the discovery of stolo and blood, and blood, and blood,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better burghis crime,>\n",
            "\n",
            "Epoch 47/100\n",
            "51/51 - 186s - loss: 0.4888 - val_loss: 0.6954\n",
            "Epoch 48/100\n",
            "51/51 - 186s - loss: 0.4853 - val_loss: 0.6990\n",
            "Epoch 49/100\n",
            "51/51 - 186s - loss: 0.4826 - val_loss: 0.6998\n",
            "Epoch 50/100\n",
            "51/51 - 185s - loss: 0.4803 - val_loss: 0.6959\n",
            "Epoch 51/100\n",
            "51/51 - 186s - loss: 0.4758 - val_loss: 0.6957\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the patry of the patryans managers and managers and managers and managures and managers and managers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very fuculty infinding a meany, and he had very few inful place inful place inful place in the worllationshipled closed greacoped.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <meanwhile, the discovery appes to and blood, the discovery appus fattered with pis ftory appuse of rains,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better burghth, could for no sinpite.>\n",
            "\n",
            "Epoch 52/100\n",
            "51/51 - 186s - loss: 0.4708 - val_loss: 0.6976\n",
            "Epoch 53/100\n",
            "51/51 - 187s - loss: 0.4668 - val_loss: 0.7009\n",
            "Epoch 54/100\n",
            "51/51 - 187s - loss: 0.4643 - val_loss: 0.6916\n",
            "Epoch 55/100\n",
            "51/51 - 186s - loss: 0.4628 - val_loss: 0.6928\n",
            "Epoch 56/100\n",
            "51/51 - 186s - loss: 0.4599 - val_loss: 0.6949\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the same angers and sepuned one of the patric fund angers and, one of the suppatric fund angers and magers and managers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he appeared of gread any appeared of gread any, and he appeareding a meear do have greading a meear do have greading a meear pppeat gry teatedininininininininin cld clt>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <meanwhil, and blood, the discovery of punians tolon, and blood, the discovery of punial and blood, and blood, and blood,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better burite for no sinptite for no sinptiter burite what better burghis crime.>\n",
            "\n",
            "Epoch 57/100\n",
            "51/51 - 186s - loss: 0.4571 - val_loss: 0.6938\n",
            "Epoch 58/100\n",
            "51/51 - 187s - loss: 0.4532 - val_loss: 0.6957\n",
            "Epoch 59/100\n",
            "51/51 - 187s - loss: 0.4500 - val_loss: 0.6939\n",
            "Epoch 60/100\n",
            "51/51 - 187s - loss: 0.4469 - val_loss: 0.6912\n",
            "Epoch 61/100\n",
            "51/51 - 186s - loss: 0.4425 - val_loss: 0.6958\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the supporters, and supporters, and one of the supporters, and one of the supporters, and,>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very few ith other pearid doo have greal place in the had very few inful place in the worlationsionships with other peace waneary ped.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <meanwife spattered wil, the dispattered with pistovery of punifestory of pewil,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better burth, crime,>\n",
            "\n",
            "Epoch 62/100\n",
            "51/51 - 186s - loss: 0.4370 - val_loss: 0.6950\n",
            "Epoch 63/100\n",
            "51/51 - 185s - loss: 0.4331 - val_loss: 0.6928\n",
            "Epoch 64/100\n",
            "51/51 - 186s - loss: 0.4307 - val_loss: 0.6985\n",
            "Epoch 65/100\n",
            "51/51 - 185s - loss: 0.4284 - val_loss: 0.6971\n",
            "Epoch 66/100\n",
            "51/51 - 187s - loss: 0.4264 - val_loss: 0.6937\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the supporters, and and supporters, and and supporters and, one of the supptrers and managers and,>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very fuculty in finding ameate closerationsionshic with other peared ohave greate fuw ith other peare difficulty in amme cldace gry gry at,>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <meanwil, the discovery appes tof reigns, and blood, and blod the discovery appeciscovery appeciscovery peumen blod and blod and blod,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better burith for no son son son son son son son son son son son son son son son son son son son sinpite.>\n",
            "\n",
            "Epoch 67/100\n",
            "51/51 - 186s - loss: 0.4236 - val_loss: 0.6914\n",
            "Epoch 68/100\n",
            "51/51 - 186s - loss: 0.4200 - val_loss: 0.6960\n",
            "Epoch 69/100\n",
            "51/51 - 186s - loss: 0.4163 - val_loss: 0.6911\n",
            "Epoch 70/100\n",
            "51/51 - 186s - loss: 0.4133 - val_loss: 0.6883\n",
            "Epoch 71/100\n",
            "51/51 - 187s - loss: 0.4110 - val_loss: 0.6891\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the patrers, and managers and managers and managers and managers and, one of the supporters, and,>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had very few inful place in the had very few inful place in meeate close relationships with other peared dified any, and he had vany ce wored.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <meanwhil, and blood, the discovery of pistol and blood, the discovery of puman blod the discovery of puman blod and blood, the blod and,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better burg hat are his crimpite.>\n",
            "\n",
            "Epoch 72/100\n",
            "51/51 - 188s - loss: 0.4085 - val_loss: 0.6894\n",
            "Epoch 73/100\n",
            "51/51 - 188s - loss: 0.4061 - val_loss: 0.6878\n",
            "Epoch 74/100\n",
            "51/51 - 187s - loss: 0.4039 - val_loss: 0.6855\n",
            "Epoch 75/100\n",
            "51/51 - 188s - loss: 0.4026 - val_loss: 0.6914\n",
            "Epoch 76/100\n",
            "51/51 - 185s - loss: 0.4018 - val_loss: 0.6921\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the same antray antray antray and managers and managers and managers and managers and managers and.>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had vicolds relationships werelationships werelationships werelationships werelationships werelationships werelationships with gred.>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <meanwil, the discovery of piscovery of pund blod the discovery of pew the discovery of pun blod the discovery of pew tho dis>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of better burg his crimpetter burgh crime.>\n",
            "\n",
            "Epoch 77/100\n",
            "51/51 - 186s - loss: 0.4002 - val_loss: 0.6903\n",
            "Epoch 78/100\n",
            "51/51 - 186s - loss: 0.3987 - val_loss: 0.6871\n",
            "Epoch 79/100\n",
            "51/51 - 187s - loss: 0.3962 - val_loss: 0.6899\n",
            "Epoch 80/100\n",
            "51/51 - 187s - loss: 0.3939 - val_loss: 0.6947\n",
            "Epoch 81/100\n",
            "51/51 - 186s - loss: 0.3921 - val_loss: 0.6961\n",
            "target:     <of the st. ann#s society, and one of the supporters and managers of the patriotic fund.>\n",
            "prediction: <of the same an, and supporters, one of the supporters, one of the supporters, one of the supporters, one of the supporters,>\n",
            "\n",
            "target:     <he had very few, if any, close relationships with other people and he appeared to have great difficulty in finding a meaningful place in the world.>\n",
            "prediction: <he had vary few it any fuculty infined a vary few infinded ifinding a meaning fuinding a meat close in the worly in fuind a ppeare dified anny a clrot,>\n",
            "\n",
            "target:     <meanwhile the discovery of pistol and knife spattered with human blood and brains>\n",
            "prediction: <meanwhile, and blood, the discovery of pistol and blood, and blood, and blood, and blood, and blood, and blood, and brains,>\n",
            "\n",
            "target:     <the man of better birth could hope for no sympathy, whatever his crime.>\n",
            "prediction: <the man of burth, crime, better his crime,>\n",
            "\n",
            "Epoch 82/100\n",
            "51/51 - 187s - loss: 0.3909 - val_loss: 0.6921\n",
            "Epoch 83/100\n",
            "51/51 - 186s - loss: 0.3884 - val_loss: 0.6893\n",
            "Epoch 84/100\n",
            "51/51 - 186s - loss: 0.3861 - val_loss: 0.6889\n",
            "33/33 [==============================] - 2s 63ms/step - loss: 0.6855\n",
            "0.6854836940765381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7MiiJfGK0mQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}