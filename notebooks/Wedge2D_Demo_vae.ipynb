{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wedge2D Demo vae",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR7WzG3MgyCn"
      },
      "source": [
        "# Variational AutoEncoder\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2020/05/03<br>\n",
        "**Last modified:** 2020/05/03<br>\n",
        "**Description:** Convolutional Variational AutoEncoder (VAE) trained on MNIST digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEhE5HJBHNza"
      },
      "source": [
        "Some displays stripped so that this would render in github."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85-PZYUJgyCv"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86nsTbi4gyCv"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pQby0nYgyCw"
      },
      "source": [
        "## Create a sampling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lRI-799gyCw"
      },
      "source": [
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klu8ok7OgyCx"
      },
      "source": [
        "## Build the encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiC93lCdgyCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4437f31-e727-4f6c-e2e8-6564403047ca"
      },
      "source": [
        "latent_dim = 2\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 14, 14, 32)   320         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 7, 7, 64)     18496       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 3136)         0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 16)           50192       flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 2)            34          dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 2)            34          dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 69,076\n",
            "Trainable params: 69,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDw3SaOrgyCx"
      },
      "source": [
        "## Build the decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WiZYBesgyCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d455e4-4b25-4563-cf55-f0fbb30e8fc8"
      },
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3136)              9408      \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       \n",
            "=================================================================\n",
            "Total params: 65,089\n",
            "Trainable params: 65,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j7yR_XNgyCy"
      },
      "source": [
        "## Define the VAE as a `Model` with a custom `train_step`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODHP5agtgyCy"
      },
      "source": [
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdVsDW4lgyCz"
      },
      "source": [
        "## Train the VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNS0NXxMgyCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd6cbf9-76b5-4081-9490-571d77a9a733"
      },
      "source": [
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(mnist_digits, epochs=30, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/30\n",
            "137/137 [==============================] - 18s 16ms/step - loss: 354.4335 - reconstruction_loss: 265.0009 - kl_loss: 1.0616\n",
            "Epoch 2/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 204.3465 - reconstruction_loss: 199.8033 - kl_loss: 2.3368\n",
            "Epoch 3/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 197.1943 - reconstruction_loss: 192.4391 - kl_loss: 3.4499\n",
            "Epoch 4/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 190.3133 - reconstruction_loss: 181.9109 - kl_loss: 5.2165\n",
            "Epoch 5/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 179.1390 - reconstruction_loss: 171.6127 - kl_loss: 5.9116\n",
            "Epoch 6/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 173.4254 - reconstruction_loss: 166.2745 - kl_loss: 6.0830\n",
            "Epoch 7/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 169.7049 - reconstruction_loss: 162.8939 - kl_loss: 6.2022\n",
            "Epoch 8/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 167.3687 - reconstruction_loss: 161.0009 - kl_loss: 6.2934\n",
            "Epoch 9/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 166.4805 - reconstruction_loss: 159.5932 - kl_loss: 6.3780\n",
            "Epoch 10/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 165.0293 - reconstruction_loss: 158.3517 - kl_loss: 6.4200\n",
            "Epoch 11/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 164.1589 - reconstruction_loss: 157.3712 - kl_loss: 6.4750\n",
            "Epoch 12/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 163.0041 - reconstruction_loss: 156.4971 - kl_loss: 6.5205\n",
            "Epoch 13/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 162.4557 - reconstruction_loss: 155.6030 - kl_loss: 6.5843\n",
            "Epoch 14/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 161.4890 - reconstruction_loss: 154.8322 - kl_loss: 6.6146\n",
            "Epoch 15/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 160.8745 - reconstruction_loss: 154.1476 - kl_loss: 6.6794\n",
            "Epoch 16/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 160.5838 - reconstruction_loss: 153.4912 - kl_loss: 6.6791\n",
            "Epoch 17/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 159.5503 - reconstruction_loss: 152.8803 - kl_loss: 6.7413\n",
            "Epoch 18/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 159.5089 - reconstruction_loss: 152.4364 - kl_loss: 6.7557\n",
            "Epoch 19/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 158.4302 - reconstruction_loss: 151.8952 - kl_loss: 6.7716\n",
            "Epoch 20/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 158.0538 - reconstruction_loss: 151.4847 - kl_loss: 6.7897\n",
            "Epoch 21/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 157.7007 - reconstruction_loss: 150.9568 - kl_loss: 6.8320\n",
            "Epoch 22/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 157.6491 - reconstruction_loss: 150.6003 - kl_loss: 6.8165\n",
            "Epoch 23/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 157.0917 - reconstruction_loss: 150.3007 - kl_loss: 6.8351\n",
            "Epoch 24/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.7166 - reconstruction_loss: 150.0719 - kl_loss: 6.8576\n",
            "Epoch 25/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.6634 - reconstruction_loss: 149.6307 - kl_loss: 6.8637\n",
            "Epoch 26/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.2728 - reconstruction_loss: 149.4765 - kl_loss: 6.8534\n",
            "Epoch 27/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.1643 - reconstruction_loss: 149.1288 - kl_loss: 6.8766\n",
            "Epoch 28/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.7272 - reconstruction_loss: 148.7725 - kl_loss: 6.8691\n",
            "Epoch 29/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.6009 - reconstruction_loss: 148.7586 - kl_loss: 6.8909\n",
            "Epoch 30/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.0260 - reconstruction_loss: 148.4204 - kl_loss: 6.8831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f600c806910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvh6C-GBg-dG"
      },
      "source": [
        "# WedgeDropout2D\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Caz7ekLRhBor",
        "outputId": "fca44d1d-61cc-491f-fab3-9f24c5411309"
      },
      "source": [
        "!pip uninstall -y keras-wedge-dropout\n",
        "!pip install -q git+https://github.com/LanceNorskog/keras-wedge.git\n",
        "import numpy as np\n",
        "from keras_wedge_dropout import WedgeDropout2D"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping keras-wedge-dropout as it is not installed.\u001b[0m\n",
            "  Building wheel for keras-wedge-dropout (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yznz6eQhBxb"
      },
      "source": [
        "latent_dim = 2\n",
        "\n",
        "def get_encoder(wedge=False):\n",
        "    encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    if wedge:\n",
        "        x = WedgeDropout2D(similarity=0.65, batchwise=True, norm=True)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(16, activation=\"relu\")(x)\n",
        "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    encoder.summary()\n",
        "    return encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zn2V_nRhB7N"
      },
      "source": [
        "def get_decoder(wedge=False):\n",
        "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "    x = layers.Reshape((7, 7, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    if wedge:\n",
        "        x = WedgeDropout2D(similarity=0.65, batchwise=True, norm=True)(x)\n",
        "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "    return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKgnOsfQhKJf",
        "outputId": "1947083f-adb8-41b9-a9af-fadd7f3ed4f4"
      },
      "source": [
        "vae = VAE(get_encoder(True), get_decoder(False))\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(mnist_digits, epochs=30, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WedgeDropout2D.build: input_shape: (None, 7, 7, 64)\n",
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 14, 14, 32)   320         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 7, 7, 64)     18496       conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "wedge_dropout2d (WedgeDropout2D (None, 7, 7, 64)     0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 3136)         0           wedge_dropout2d[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 16)           50192       flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 2)            34          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 2)            34          dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampling_1 (Sampling)           (None, 2)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 69,076\n",
            "Trainable params: 69,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3136)              9408      \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 32)        18464     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 1)         289       \n",
            "=================================================================\n",
            "Total params: 65,089\n",
            "Trainable params: 65,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "137/137 [==============================] - 3s 16ms/step - loss: 335.2796 - reconstruction_loss: 255.3511 - kl_loss: 0.9891\n",
            "Epoch 2/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 205.4428 - reconstruction_loss: 198.1426 - kl_loss: 1.8903\n",
            "Epoch 3/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 182.3069 - reconstruction_loss: 174.1283 - kl_loss: 4.5736\n",
            "Epoch 4/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 172.1731 - reconstruction_loss: 166.0866 - kl_loss: 4.8612\n",
            "Epoch 5/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 167.9797 - reconstruction_loss: 162.1329 - kl_loss: 4.9539\n",
            "Epoch 6/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 165.1778 - reconstruction_loss: 159.6535 - kl_loss: 5.0780\n",
            "Epoch 7/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 163.0070 - reconstruction_loss: 157.3972 - kl_loss: 5.2352\n",
            "Epoch 8/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 161.4201 - reconstruction_loss: 155.9055 - kl_loss: 5.3323\n",
            "Epoch 9/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 159.9988 - reconstruction_loss: 154.7629 - kl_loss: 5.4066\n",
            "Epoch 10/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 159.4184 - reconstruction_loss: 153.9672 - kl_loss: 5.4608\n",
            "Epoch 11/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 158.7378 - reconstruction_loss: 153.4525 - kl_loss: 5.5141\n",
            "Epoch 12/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 158.6984 - reconstruction_loss: 152.7067 - kl_loss: 5.5614\n",
            "Epoch 13/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 158.0624 - reconstruction_loss: 152.2442 - kl_loss: 5.5922\n",
            "Epoch 14/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 157.7839 - reconstruction_loss: 151.8604 - kl_loss: 5.6210\n",
            "Epoch 15/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.6651 - reconstruction_loss: 151.3053 - kl_loss: 5.6715\n",
            "Epoch 16/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.7230 - reconstruction_loss: 150.8985 - kl_loss: 5.6842\n",
            "Epoch 17/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.5815 - reconstruction_loss: 150.6227 - kl_loss: 5.7072\n",
            "Epoch 18/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.3585 - reconstruction_loss: 150.2640 - kl_loss: 5.7233\n",
            "Epoch 19/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.4454 - reconstruction_loss: 149.9602 - kl_loss: 5.7583\n",
            "Epoch 20/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.4966 - reconstruction_loss: 149.5927 - kl_loss: 5.7856\n",
            "Epoch 21/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.8732 - reconstruction_loss: 149.3341 - kl_loss: 5.8166\n",
            "Epoch 22/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.0373 - reconstruction_loss: 149.1283 - kl_loss: 5.8230\n",
            "Epoch 23/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.4832 - reconstruction_loss: 148.8044 - kl_loss: 5.8464\n",
            "Epoch 24/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.2729 - reconstruction_loss: 148.6429 - kl_loss: 5.8670\n",
            "Epoch 25/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.2821 - reconstruction_loss: 148.2997 - kl_loss: 5.8957\n",
            "Epoch 26/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.0337 - reconstruction_loss: 148.1371 - kl_loss: 5.9067\n",
            "Epoch 27/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.1568 - reconstruction_loss: 148.0385 - kl_loss: 5.9108\n",
            "Epoch 28/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.3812 - reconstruction_loss: 147.8633 - kl_loss: 5.9204\n",
            "Epoch 29/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.6159 - reconstruction_loss: 147.5670 - kl_loss: 5.9496\n",
            "Epoch 30/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.5211 - reconstruction_loss: 147.3445 - kl_loss: 5.9797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5f2e2cb790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDOy1W5whtDB",
        "outputId": "31678dfc-d44b-4a51-f9b1-e1137924e196"
      },
      "source": [
        "vae = VAE(get_encoder(False), get_decoder(True))\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(mnist_digits, epochs=30, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 14, 14, 32)   320         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 7, 7, 64)     18496       conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 3136)         0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 16)           50192       flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 2)            34          dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 2)            34          dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampling_2 (Sampling)           (None, 2)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 69,076\n",
            "Trainable params: 69,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WedgeDropout2D.build: input_shape: (None, 28, 28, 32)\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3136)              9408      \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTr (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 32)        18464     \n",
            "_________________________________________________________________\n",
            "wedge_dropout2d_1 (WedgeDrop (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 1)         289       \n",
            "=================================================================\n",
            "Total params: 65,089\n",
            "Trainable params: 65,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "137/137 [==============================] - 3s 16ms/step - loss: 329.7349 - reconstruction_loss: 247.0791 - kl_loss: 4.0092\n",
            "Epoch 2/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 199.7466 - reconstruction_loss: 193.9061 - kl_loss: 3.3893\n",
            "Epoch 3/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 191.2563 - reconstruction_loss: 187.6477 - kl_loss: 2.5938\n",
            "Epoch 4/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 188.0847 - reconstruction_loss: 184.6885 - kl_loss: 2.4838\n",
            "Epoch 5/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 184.9201 - reconstruction_loss: 181.1401 - kl_loss: 2.6621\n",
            "Epoch 6/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 180.6087 - reconstruction_loss: 174.8233 - kl_loss: 3.5099\n",
            "Epoch 7/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 169.8453 - reconstruction_loss: 163.4113 - kl_loss: 4.8205\n",
            "Epoch 8/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 165.1060 - reconstruction_loss: 159.2359 - kl_loss: 5.0884\n",
            "Epoch 9/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 162.6587 - reconstruction_loss: 156.9523 - kl_loss: 5.2467\n",
            "Epoch 10/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 161.0172 - reconstruction_loss: 155.4736 - kl_loss: 5.3425\n",
            "Epoch 11/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 159.9568 - reconstruction_loss: 154.2249 - kl_loss: 5.4308\n",
            "Epoch 12/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 158.9052 - reconstruction_loss: 153.2298 - kl_loss: 5.4876\n",
            "Epoch 13/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 158.2387 - reconstruction_loss: 152.3902 - kl_loss: 5.5543\n",
            "Epoch 14/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 157.4926 - reconstruction_loss: 151.6932 - kl_loss: 5.5968\n",
            "Epoch 15/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.8431 - reconstruction_loss: 151.0643 - kl_loss: 5.6438\n",
            "Epoch 16/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.5886 - reconstruction_loss: 150.5154 - kl_loss: 5.6786\n",
            "Epoch 17/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.6319 - reconstruction_loss: 150.0251 - kl_loss: 5.7107\n",
            "Epoch 18/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.1621 - reconstruction_loss: 149.5732 - kl_loss: 5.7584\n",
            "Epoch 19/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.9240 - reconstruction_loss: 149.2439 - kl_loss: 5.7709\n",
            "Epoch 20/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.5709 - reconstruction_loss: 148.8649 - kl_loss: 5.8112\n",
            "Epoch 21/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.2930 - reconstruction_loss: 148.5595 - kl_loss: 5.8308\n",
            "Epoch 22/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.4613 - reconstruction_loss: 148.3438 - kl_loss: 5.8429\n",
            "Epoch 23/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.7126 - reconstruction_loss: 147.9808 - kl_loss: 5.8808\n",
            "Epoch 24/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.4419 - reconstruction_loss: 147.7480 - kl_loss: 5.8912\n",
            "Epoch 25/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.8480 - reconstruction_loss: 147.4406 - kl_loss: 5.9022\n",
            "Epoch 26/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.3315 - reconstruction_loss: 147.2635 - kl_loss: 5.9262\n",
            "Epoch 27/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.2269 - reconstruction_loss: 147.0275 - kl_loss: 5.9333\n",
            "Epoch 28/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 152.7758 - reconstruction_loss: 146.8346 - kl_loss: 5.9603\n",
            "Epoch 29/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 152.5925 - reconstruction_loss: 146.6544 - kl_loss: 5.9714\n",
            "Epoch 30/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 152.5648 - reconstruction_loss: 146.5290 - kl_loss: 6.0036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5f0d5ee650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYjRitPihtOB",
        "outputId": "a867d57d-a726-4d7a-90f4-f72a5939e576"
      },
      "source": [
        "vae = VAE(get_encoder(True), get_decoder(True))\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(mnist_digits, epochs=30, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WedgeDropout2D.build: input_shape: (None, 7, 7, 64)\n",
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 14, 14, 32)   320         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 7, 7, 64)     18496       conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "wedge_dropout2d_2 (WedgeDropout (None, 7, 7, 64)     0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 3136)         0           wedge_dropout2d_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 16)           50192       flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 2)            34          dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 2)            34          dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampling_3 (Sampling)           (None, 2)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 69,076\n",
            "Trainable params: 69,076\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WedgeDropout2D.build: input_shape: (None, 28, 28, 32)\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3136)              9408      \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTr (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DT (None, 28, 28, 32)        18464     \n",
            "_________________________________________________________________\n",
            "wedge_dropout2d_3 (WedgeDrop (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DT (None, 28, 28, 1)         289       \n",
            "=================================================================\n",
            "Total params: 65,089\n",
            "Trainable params: 65,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "137/137 [==============================] - 3s 16ms/step - loss: 333.0508 - reconstruction_loss: 251.4424 - kl_loss: 1.9213\n",
            "Epoch 2/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 201.9026 - reconstruction_loss: 196.8194 - kl_loss: 2.6949\n",
            "Epoch 3/30\n",
            "137/137 [==============================] - 2s 15ms/step - loss: 194.3057 - reconstruction_loss: 190.7280 - kl_loss: 3.0496\n",
            "Epoch 4/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 191.8805 - reconstruction_loss: 188.3786 - kl_loss: 2.9473\n",
            "Epoch 5/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 188.9561 - reconstruction_loss: 185.4742 - kl_loss: 3.0268\n",
            "Epoch 6/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 185.9795 - reconstruction_loss: 181.7563 - kl_loss: 3.3524\n",
            "Epoch 7/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 176.8421 - reconstruction_loss: 169.1094 - kl_loss: 5.1118\n",
            "Epoch 8/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 168.9936 - reconstruction_loss: 162.6932 - kl_loss: 5.7151\n",
            "Epoch 9/30\n",
            "137/137 [==============================] - 2s 15ms/step - loss: 166.2222 - reconstruction_loss: 159.4491 - kl_loss: 5.8855\n",
            "Epoch 10/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 163.5040 - reconstruction_loss: 157.1359 - kl_loss: 6.0129\n",
            "Epoch 11/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 161.6603 - reconstruction_loss: 155.3300 - kl_loss: 6.1434\n",
            "Epoch 12/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 160.4669 - reconstruction_loss: 154.1888 - kl_loss: 6.1985\n",
            "Epoch 13/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 159.7656 - reconstruction_loss: 153.2147 - kl_loss: 6.2679\n",
            "Epoch 14/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 158.6999 - reconstruction_loss: 152.3623 - kl_loss: 6.3242\n",
            "Epoch 15/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 158.2632 - reconstruction_loss: 151.7825 - kl_loss: 6.3601\n",
            "Epoch 16/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 157.6708 - reconstruction_loss: 151.2468 - kl_loss: 6.3830\n",
            "Epoch 17/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 157.2459 - reconstruction_loss: 150.6222 - kl_loss: 6.3999\n",
            "Epoch 18/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 157.0688 - reconstruction_loss: 150.2591 - kl_loss: 6.4449\n",
            "Epoch 19/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 156.3091 - reconstruction_loss: 149.7665 - kl_loss: 6.4561\n",
            "Epoch 20/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.9169 - reconstruction_loss: 149.3275 - kl_loss: 6.4594\n",
            "Epoch 21/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.4822 - reconstruction_loss: 149.0941 - kl_loss: 6.4829\n",
            "Epoch 22/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 155.4679 - reconstruction_loss: 148.6278 - kl_loss: 6.5031\n",
            "Epoch 23/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.4879 - reconstruction_loss: 148.3086 - kl_loss: 6.5114\n",
            "Epoch 24/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.3742 - reconstruction_loss: 147.9478 - kl_loss: 6.5393\n",
            "Epoch 25/30\n",
            "137/137 [==============================] - 2s 15ms/step - loss: 154.1961 - reconstruction_loss: 147.8447 - kl_loss: 6.5538\n",
            "Epoch 26/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 154.0746 - reconstruction_loss: 147.4644 - kl_loss: 6.5610\n",
            "Epoch 27/30\n",
            "137/137 [==============================] - 2s 15ms/step - loss: 153.9367 - reconstruction_loss: 147.2852 - kl_loss: 6.5710\n",
            "Epoch 28/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.7555 - reconstruction_loss: 147.0461 - kl_loss: 6.5651\n",
            "Epoch 29/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.3727 - reconstruction_loss: 146.7243 - kl_loss: 6.5819\n",
            "Epoch 30/30\n",
            "137/137 [==============================] - 2s 16ms/step - loss: 153.1087 - reconstruction_loss: 146.5681 - kl_loss: 6.6054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5f2e3658d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}