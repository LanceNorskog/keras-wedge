{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WedgeDropout1D pretrained_word_embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4rgDbkjmmuY"
      },
      "source": [
        "# Using pre-trained word embeddings\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2020/05/05<br>\n",
        "**Last modified:** 2020/05/05<br>\n",
        "**Description:** Text classification on the Newsgroup20 dataset using pre-trained GloVe word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh5YTrCFmmua"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmDtk8Bxm9ft",
        "outputId": "eabcdf2e-58b6-444f-b10e-c5c67ba0d2f8"
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install -qq tensorflow==2.5.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.5.0\n",
            "Uninstalling tensorflow-2.5.0:\n",
            "  Successfully uninstalled tensorflow-2.5.0\n",
            "\u001b[K     |████████████████████████████████| 454.3 MB 13 kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-_7RiWemmub"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JmFeQvZmmub"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we show how to train a text classification model that uses pre-trained\n",
        "word embeddings.\n",
        "\n",
        "We'll work with the Newsgroup20 dataset, a set of 20,000 message board messages\n",
        "belonging to 20 different topic categories.\n",
        "\n",
        "For the pre-trained word embeddings, we'll use\n",
        "[GloVe embeddings](http://nlp.stanford.edu/projects/glove/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqLgGmqTmmuc"
      },
      "source": [
        "## Download the Newsgroup20 data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glE3mUFFmmuc",
        "outputId": "af6eac9c-d585-4df8-c265-8712859077b6"
      },
      "source": [
        "data_path = keras.utils.get_file(\n",
        "    \"news20.tar.gz\",\n",
        "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
        "    untar=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
            "17334272/17329808 [==============================] - 11s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsHWZqPNmmuc"
      },
      "source": [
        "## Let's take a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoJOu7j6mmud",
        "outputId": "a53ac2c7-d487-480d-c6f2-e371f4bf59ae"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
        "dirnames = os.listdir(data_dir)\n",
        "print(\"Number of directories:\", len(dirnames))\n",
        "print(\"Directory names:\", dirnames)\n",
        "\n",
        "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
        "print(\"Number of files in comp.graphics:\", len(fnames))\n",
        "print(\"Some example filenames:\", fnames[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of directories: 20\n",
            "Directory names: ['rec.sport.hockey', 'talk.politics.misc', 'rec.sport.baseball', 'talk.politics.guns', 'comp.windows.x', 'talk.religion.misc', 'soc.religion.christian', 'rec.motorcycles', 'alt.atheism', 'misc.forsale', 'sci.crypt', 'comp.sys.mac.hardware', 'comp.graphics', 'sci.med', 'rec.autos', 'comp.os.ms-windows.misc', 'sci.space', 'sci.electronics', 'comp.sys.ibm.pc.hardware', 'talk.politics.mideast']\n",
            "Number of files in comp.graphics: 1000\n",
            "Some example filenames: ['38484', '38646', '38980', '38712', '39039']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0waQuVmmmud"
      },
      "source": [
        "Here's a example of what one file contains:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IETPQ2Ammud",
        "outputId": "6f0e23c1-b79c-49ad-a683-9cb7dbb0e947"
      },
      "source": [
        "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Newsgroups: comp.graphics\n",
            "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
            "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
            "Subject: Looking for Brain in CAD\n",
            "Message-ID: <c285m+p@rpi.edu>\n",
            "Nntp-Posting-Host: nason110.its.rpi.edu\n",
            "Reply-To: mabusj@rpi.edu\n",
            "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
            "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
            "Lines: 7\n",
            "\n",
            "Jasen Mabus\n",
            "RPI student\n",
            "\n",
            "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
            "\n",
            "Thank you in advance,\n",
            "Jasen Mabus  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBM1iJvOmmue"
      },
      "source": [
        "As you can see, there are header lines that are leaking the file's category, either\n",
        "explicitly (the first line is literally the category name), or implicitly, e.g. via the\n",
        "`Organization` filed. Let's get rid of the headers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxF-BwE1mmue",
        "outputId": "694fe348-8144-4be4-d1e0-b05849cc902a"
      },
      "source": [
        "samples = []\n",
        "labels = []\n",
        "class_names = []\n",
        "class_index = 0\n",
        "for dirname in sorted(os.listdir(data_dir)):\n",
        "    class_names.append(dirname)\n",
        "    dirpath = data_dir / dirname\n",
        "    fnames = os.listdir(dirpath)\n",
        "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
        "    for fname in fnames:\n",
        "        fpath = dirpath / fname\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        lines = content.split(\"\\n\")\n",
        "        lines = lines[10:]\n",
        "        content = \"\\n\".join(lines)\n",
        "        samples.append(content)\n",
        "        labels.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing alt.atheism, 1000 files found\n",
            "Processing comp.graphics, 1000 files found\n",
            "Processing comp.os.ms-windows.misc, 1000 files found\n",
            "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
            "Processing comp.sys.mac.hardware, 1000 files found\n",
            "Processing comp.windows.x, 1000 files found\n",
            "Processing misc.forsale, 1000 files found\n",
            "Processing rec.autos, 1000 files found\n",
            "Processing rec.motorcycles, 1000 files found\n",
            "Processing rec.sport.baseball, 1000 files found\n",
            "Processing rec.sport.hockey, 1000 files found\n",
            "Processing sci.crypt, 1000 files found\n",
            "Processing sci.electronics, 1000 files found\n",
            "Processing sci.med, 1000 files found\n",
            "Processing sci.space, 1000 files found\n",
            "Processing soc.religion.christian, 997 files found\n",
            "Processing talk.politics.guns, 1000 files found\n",
            "Processing talk.politics.mideast, 1000 files found\n",
            "Processing talk.politics.misc, 1000 files found\n",
            "Processing talk.religion.misc, 1000 files found\n",
            "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "Number of samples: 19997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9II9dUm9mmue"
      },
      "source": [
        "There's actually one category that doesn't have the expected number of files, but the\n",
        "difference is small enough that the problem remains a balanced classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwEb5GFmmmuf"
      },
      "source": [
        "## Shuffle and split the data into training & validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4PGdRkwmmuf"
      },
      "source": [
        "# Shuffle the data\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples]\n",
        "val_samples = samples[-num_validation_samples:]\n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3yaeceNmmuf"
      },
      "source": [
        "## Create a vocabulary index\n",
        "\n",
        "Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n",
        "Later, we'll use the same layer instance to vectorize the samples.\n",
        "\n",
        "Our layer will only consider the top 20,000 words, and will truncate or pad sequences to\n",
        "be actually 200 tokens long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_Zx_dZXmmuf"
      },
      "source": [
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33fUX_aUmmug"
      },
      "source": [
        "You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n",
        "print the top 5 words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX5Fe-iEmmug",
        "outputId": "ee9b109a-3ff7-4711-c8d8-b37370e83e4e"
      },
      "source": [
        "vectorizer.get_vocabulary()[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'to', 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vctsceiqmmug"
      },
      "source": [
        "Let's vectorize a test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYZwOh5nmmug",
        "outputId": "9394cdcd-8db0-4ac2-d9a6-ac5eb6f2b2e2"
      },
      "source": [
        "output = vectorizer([[\"the cat sat on the mat\"]])\n",
        "output.numpy()[0, :6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2, 3545, 1717,   15,    2, 6040])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kju0msGmmug"
      },
      "source": [
        "As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n",
        "word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n",
        "reserved for \"out of vocabulary\" tokens.\n",
        "\n",
        "Here's a dict mapping words to their indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDQLzYHSmmuh"
      },
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOFZqjMGmmuh"
      },
      "source": [
        "As you can see, we obtain the same encoding as above for our test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl2M-UPYmmuh",
        "outputId": "58204f93-e0b7-4d1c-8eef-1261668b719a"
      },
      "source": [
        "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "[word_index[w] for w in test]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3545, 1717, 15, 2, 6040]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuRHr4Bpmmui"
      },
      "source": [
        "## Load pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUqNI-2mmmui"
      },
      "source": [
        "Let's download pre-trained GloVe embeddings (a 822M zip file).\n",
        "\n",
        "You'll need to run the following commands:\n",
        "\n",
        "```\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKK5fs9QoYcF"
      },
      "source": [
        "!wget -q -nc http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61J-d1KGmmui"
      },
      "source": [
        "The archive contains text-encoded vectors of various sizes: 50-dimensional,\n",
        "100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n",
        "\n",
        "Let's make a dict mapping words (strings) to their NumPy vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmgqeNSWmmuj",
        "outputId": "473eecc3-ecce-43a2-f0af-0a0adc12f9f2"
      },
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\"\n",
        ")\n",
        "path_to_glove_file = os.path.join(\n",
        "    \"/content/glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BExB_dWammuj"
      },
      "source": [
        "Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
        "`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
        "vector for the word of index `i` in our `vectorizer`'s vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdx5mkR6mmuj",
        "outputId": "9b9da52e-bebd-4e83-a979-dc847e1bb310"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 17984 words (2016 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g7MEDAMmmuj"
      },
      "source": [
        "Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n",
        "\n",
        "Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n",
        "update them during training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlMqsZSemmuk"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl_NJUxXmmuk"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "A simple 1D convnet with global max pooling and a classifier at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz2Ktbujmmuk",
        "outputId": "b4f1fbbc-59bd-41c4-fc90-967da495a15a"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 100)         2000200   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 128)         64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                2580      \n",
            "=================================================================\n",
            "Total params: 2,247,516\n",
            "Trainable params: 247,316\n",
            "Non-trainable params: 2,000,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PjRBKCOmmuk"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n",
        "are right-padded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M6Qc2J0mmuk"
      },
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWci0NAlmmul"
      },
      "source": [
        "We use categorical crossentropy as our loss since we're doing softmax classification.\n",
        "Moreover, we use `sparse_categorical_crossentropy` since our labels are integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n_8jQ10mmul",
        "outputId": "d7667e23-8182-4989-ea78-bc43f85e9731"
      },
      "source": [
        "batch_size = 512\n",
        "epochs = 200\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val), callbacks=[early_stopping])\n",
        "print(model.evaluate(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 17s 506ms/step - loss: 2.9485 - acc: 0.0801 - val_loss: 2.8277 - val_acc: 0.1310\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 16s 498ms/step - loss: 2.5609 - acc: 0.1616 - val_loss: 2.4809 - val_acc: 0.1520\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 16s 497ms/step - loss: 2.2392 - acc: 0.2342 - val_loss: 2.1444 - val_acc: 0.2546\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 16s 498ms/step - loss: 1.9893 - acc: 0.3139 - val_loss: 1.8857 - val_acc: 0.3498\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 16s 496ms/step - loss: 1.7796 - acc: 0.3962 - val_loss: 1.6120 - val_acc: 0.4244\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 16s 495ms/step - loss: 1.6129 - acc: 0.4494 - val_loss: 1.5494 - val_acc: 0.4611\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 1.4603 - acc: 0.4993 - val_loss: 1.3523 - val_acc: 0.5239\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 16s 497ms/step - loss: 1.3724 - acc: 0.5314 - val_loss: 1.3035 - val_acc: 0.5481\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 16s 499ms/step - loss: 1.2648 - acc: 0.5726 - val_loss: 1.3277 - val_acc: 0.5456\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 16s 495ms/step - loss: 1.1968 - acc: 0.5912 - val_loss: 1.2931 - val_acc: 0.5546\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 16s 494ms/step - loss: 1.1202 - acc: 0.6175 - val_loss: 1.5836 - val_acc: 0.4711\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 16s 492ms/step - loss: 1.0782 - acc: 0.6357 - val_loss: 1.1295 - val_acc: 0.6079\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 16s 491ms/step - loss: 1.0170 - acc: 0.6541 - val_loss: 1.1235 - val_acc: 0.6237\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 16s 491ms/step - loss: 0.9576 - acc: 0.6718 - val_loss: 1.2314 - val_acc: 0.5774\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 16s 494ms/step - loss: 0.9111 - acc: 0.6865 - val_loss: 1.1996 - val_acc: 0.6059\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 16s 490ms/step - loss: 0.8786 - acc: 0.6964 - val_loss: 1.1664 - val_acc: 0.6094\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 16s 491ms/step - loss: 0.8246 - acc: 0.7128 - val_loss: 1.1042 - val_acc: 0.6277\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.7845 - acc: 0.7284 - val_loss: 1.2331 - val_acc: 0.5969\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 16s 490ms/step - loss: 0.7580 - acc: 0.7353 - val_loss: 1.0099 - val_acc: 0.6527\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 16s 496ms/step - loss: 0.6877 - acc: 0.7612 - val_loss: 1.0998 - val_acc: 0.6452\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.6853 - acc: 0.7606 - val_loss: 1.1450 - val_acc: 0.6337\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 16s 495ms/step - loss: 0.6472 - acc: 0.7746 - val_loss: 1.2129 - val_acc: 0.6382\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.6326 - acc: 0.7812 - val_loss: 1.0662 - val_acc: 0.6577\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 16s 495ms/step - loss: 0.5656 - acc: 0.8039 - val_loss: 1.0385 - val_acc: 0.6687\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 16s 517ms/step - loss: 0.5599 - acc: 0.8032 - val_loss: 1.1996 - val_acc: 0.6409\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 21s 638ms/step - loss: 0.5203 - acc: 0.8194 - val_loss: 1.1137 - val_acc: 0.6734\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 16s 494ms/step - loss: 0.4974 - acc: 0.8280 - val_loss: 1.1879 - val_acc: 0.6467\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 16s 496ms/step - loss: 0.4510 - acc: 0.8430 - val_loss: 1.0731 - val_acc: 0.6742\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 16s 495ms/step - loss: 0.4542 - acc: 0.8404 - val_loss: 0.9771 - val_acc: 0.6912\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 16s 494ms/step - loss: 0.3960 - acc: 0.8615 - val_loss: 1.3192 - val_acc: 0.6374\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 16s 492ms/step - loss: 0.3821 - acc: 0.8687 - val_loss: 1.2192 - val_acc: 0.6657\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.3664 - acc: 0.8750 - val_loss: 1.3196 - val_acc: 0.6612\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 16s 492ms/step - loss: 0.3333 - acc: 0.8861 - val_loss: 1.2149 - val_acc: 0.6569\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 16s 492ms/step - loss: 0.3350 - acc: 0.8885 - val_loss: 1.1778 - val_acc: 0.6764\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 16s 489ms/step - loss: 0.2942 - acc: 0.9019 - val_loss: 1.3475 - val_acc: 0.6629\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 16s 496ms/step - loss: 0.3168 - acc: 0.8914 - val_loss: 1.0813 - val_acc: 0.7054\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 16s 489ms/step - loss: 0.2716 - acc: 0.9121 - val_loss: 1.2992 - val_acc: 0.6724\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 16s 491ms/step - loss: 0.2580 - acc: 0.9149 - val_loss: 1.5780 - val_acc: 0.6559\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 16s 489ms/step - loss: 0.2656 - acc: 0.9139 - val_loss: 1.1932 - val_acc: 0.7042\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 16s 496ms/step - loss: 0.2344 - acc: 0.9246 - val_loss: 1.3984 - val_acc: 0.6709\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.2246 - acc: 0.9246 - val_loss: 1.2471 - val_acc: 0.7059\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.2349 - acc: 0.9221 - val_loss: 1.3999 - val_acc: 0.6699\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.2090 - acc: 0.9322 - val_loss: 1.4285 - val_acc: 0.6672\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.2040 - acc: 0.9334 - val_loss: 1.5241 - val_acc: 0.6547\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 16s 498ms/step - loss: 0.2313 - acc: 0.9282 - val_loss: 1.2131 - val_acc: 0.7079\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 16s 496ms/step - loss: 0.1619 - acc: 0.9484 - val_loss: 1.3967 - val_acc: 0.6902\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.1932 - acc: 0.9393 - val_loss: 1.4222 - val_acc: 0.6867\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 16s 498ms/step - loss: 0.1782 - acc: 0.9451 - val_loss: 1.7123 - val_acc: 0.6684\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 16s 497ms/step - loss: 0.1816 - acc: 0.9407 - val_loss: 1.3502 - val_acc: 0.7084\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.1667 - acc: 0.9471 - val_loss: 1.3473 - val_acc: 0.7017\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 16s 495ms/step - loss: 0.1511 - acc: 0.9506 - val_loss: 1.4561 - val_acc: 0.7092\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 16s 494ms/step - loss: 0.1804 - acc: 0.9450 - val_loss: 1.4894 - val_acc: 0.6999\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 16s 493ms/step - loss: 0.1514 - acc: 0.9525 - val_loss: 1.5344 - val_acc: 0.6504\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 16s 494ms/step - loss: 0.1531 - acc: 0.9466 - val_loss: 1.6024 - val_acc: 0.6472\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 16s 496ms/step - loss: 0.1309 - acc: 0.9572 - val_loss: 1.7322 - val_acc: 0.6757\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 16s 495ms/step - loss: 0.1398 - acc: 0.9525 - val_loss: 2.7335 - val_acc: 0.5759\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 16s 494ms/step - loss: 0.1322 - acc: 0.9563 - val_loss: 1.6627 - val_acc: 0.6977\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 16s 496ms/step - loss: 0.1224 - acc: 0.9572 - val_loss: 1.5918 - val_acc: 0.6927\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 16s 497ms/step - loss: 0.1708 - acc: 0.9466 - val_loss: 1.6460 - val_acc: 0.6937\n",
            "125/125 [==============================] - 2s 16ms/step - loss: 0.9771 - acc: 0.6912\n",
            "[0.9770841598510742, 0.6911727786064148]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7druvO99mmul"
      },
      "source": [
        "## Export an end-to-end model\n",
        "\n",
        "Now, we may want to export a `Model` object that takes as input a string of arbitrary\n",
        "length, rather than a sequence of indices. It would make the model much more portable,\n",
        "since you wouldn't have to worry about the input preprocessing pipeline.\n",
        "\n",
        "Our `vectorizer` is actually a Keras layer, so it's simple:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bY5kJgRXmmul",
        "outputId": "72df227b-f542-4740-c873-4db2915a4ac6"
      },
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = model(x)\n",
        "end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'comp.graphics'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEq-NzcrqL-0"
      },
      "source": [
        "# WedgeDropout1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuNtIkO0rouP",
        "outputId": "99b32e2a-cc69-4f3f-884a-97d784312e08"
      },
      "source": [
        "!pip uninstall -y keras-wedge-dropout\n",
        "!pip install -q git+https://github.com/LanceNorskog/keras-wedge.git\n",
        "from keras_wedge_dropout import WedgeDropout1D"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping keras-wedge-dropout as it is not installed.\u001b[0m\n",
            "  Building wheel for keras-wedge-dropout (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLyqqpUEqVvr",
        "outputId": "00845d85-80af-46e1-aaac-7682f3bf9ba8"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.SpatialDropout1D(0.3)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = WedgeDropout1D(similarity=0.65)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "# x = layers.Dropout(0.3)(x)\n",
        "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "model_wedge = keras.Model(int_sequences_input, preds)\n",
        "model_wedge.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 100)         2000200   \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, None, 128)         64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_19 (Conv1D)           (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_20 (Conv1D)           (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "wedge_dropout1d_5 (WedgeDrop (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_6 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 20)                2580      \n",
            "=================================================================\n",
            "Total params: 2,247,516\n",
            "Trainable params: 247,316\n",
            "Non-trainable params: 2,000,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aK47Ztiqdqd",
        "outputId": "62c51030-92bf-43dc-bace-fd397277cf27"
      },
      "source": [
        "opt=tf.keras.optimizers.RMSprop(\n",
        "    learning_rate=0.0001)\n",
        "model_wedge.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"]\n",
        ")\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
        "model_wedge.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val), callbacks=[early_stopping])\n",
        "print(model_wedge.evaluate(x_val, y_val))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 20s 514ms/step - loss: 2.9940 - acc: 0.0618 - val_loss: 2.9559 - val_acc: 0.0915\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 2.9458 - acc: 0.0837 - val_loss: 2.9078 - val_acc: 0.0945\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 2.8831 - acc: 0.1031 - val_loss: 2.7958 - val_acc: 0.1155\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 2.7663 - acc: 0.1331 - val_loss: 2.6523 - val_acc: 0.1883\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 2.6238 - acc: 0.1683 - val_loss: 2.4986 - val_acc: 0.2236\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 16s 516ms/step - loss: 2.5023 - acc: 0.1911 - val_loss: 2.3817 - val_acc: 0.2343\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 2.3975 - acc: 0.2126 - val_loss: 2.2897 - val_acc: 0.2523\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 16s 514ms/step - loss: 2.3143 - acc: 0.2302 - val_loss: 2.2209 - val_acc: 0.2566\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 2.2316 - acc: 0.2497 - val_loss: 2.1308 - val_acc: 0.3101\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 2.1628 - acc: 0.2683 - val_loss: 2.0854 - val_acc: 0.2951\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 2.1084 - acc: 0.2865 - val_loss: 2.0359 - val_acc: 0.3093\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 2.0652 - acc: 0.3039 - val_loss: 2.0127 - val_acc: 0.3036\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 2.0163 - acc: 0.3072 - val_loss: 1.9487 - val_acc: 0.3521\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 16s 514ms/step - loss: 1.9758 - acc: 0.3279 - val_loss: 1.9368 - val_acc: 0.3446\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 1.9362 - acc: 0.3433 - val_loss: 1.8869 - val_acc: 0.3703\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 16s 516ms/step - loss: 1.9030 - acc: 0.3536 - val_loss: 1.8455 - val_acc: 0.3818\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 16s 514ms/step - loss: 1.8679 - acc: 0.3667 - val_loss: 1.8149 - val_acc: 0.3828\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 1.8190 - acc: 0.3826 - val_loss: 1.8002 - val_acc: 0.3863\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 1.7953 - acc: 0.3955 - val_loss: 1.7457 - val_acc: 0.4094\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 16s 516ms/step - loss: 1.7570 - acc: 0.4007 - val_loss: 1.6951 - val_acc: 0.4341\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 1.7335 - acc: 0.4092 - val_loss: 1.7096 - val_acc: 0.4036\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 1.6995 - acc: 0.4256 - val_loss: 1.6786 - val_acc: 0.4149\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 17s 519ms/step - loss: 1.6711 - acc: 0.4308 - val_loss: 1.6304 - val_acc: 0.4479\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 1.6497 - acc: 0.4415 - val_loss: 1.6046 - val_acc: 0.4536\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 16s 516ms/step - loss: 1.6155 - acc: 0.4536 - val_loss: 1.5916 - val_acc: 0.4571\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 1.5952 - acc: 0.4585 - val_loss: 1.5821 - val_acc: 0.4609\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 16s 516ms/step - loss: 1.5745 - acc: 0.4646 - val_loss: 1.5852 - val_acc: 0.4644\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 16s 514ms/step - loss: 1.5471 - acc: 0.4765 - val_loss: 1.5264 - val_acc: 0.4864\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 1.5304 - acc: 0.4847 - val_loss: 1.5393 - val_acc: 0.4746\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 1.5005 - acc: 0.4911 - val_loss: 1.4767 - val_acc: 0.5056\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 17s 517ms/step - loss: 1.4968 - acc: 0.4926 - val_loss: 1.4635 - val_acc: 0.5039\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.4679 - acc: 0.5046 - val_loss: 1.4717 - val_acc: 0.4969\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 1.4453 - acc: 0.5099 - val_loss: 1.4817 - val_acc: 0.4789\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 1.4409 - acc: 0.5169 - val_loss: 1.4202 - val_acc: 0.5214\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 16s 514ms/step - loss: 1.4226 - acc: 0.5183 - val_loss: 1.4631 - val_acc: 0.5051\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 1.4052 - acc: 0.5303 - val_loss: 1.3959 - val_acc: 0.5394\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 1.3909 - acc: 0.5334 - val_loss: 1.3652 - val_acc: 0.5366\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.3746 - acc: 0.5322 - val_loss: 1.3793 - val_acc: 0.5356\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 1.3576 - acc: 0.5461 - val_loss: 1.3769 - val_acc: 0.5356\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 1.3306 - acc: 0.5515 - val_loss: 1.3501 - val_acc: 0.5454\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.3306 - acc: 0.5506 - val_loss: 1.3274 - val_acc: 0.5436\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 1.3096 - acc: 0.5559 - val_loss: 1.3023 - val_acc: 0.5571\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.2995 - acc: 0.5646 - val_loss: 1.3286 - val_acc: 0.5536\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 1.2818 - acc: 0.5713 - val_loss: 1.3123 - val_acc: 0.5509\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 1.2769 - acc: 0.5702 - val_loss: 1.3170 - val_acc: 0.5584\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 1.2639 - acc: 0.5788 - val_loss: 1.3169 - val_acc: 0.5399\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.2445 - acc: 0.5880 - val_loss: 1.2917 - val_acc: 0.5564\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 1.2445 - acc: 0.5838 - val_loss: 1.2794 - val_acc: 0.5641\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 1.2242 - acc: 0.5924 - val_loss: 1.2630 - val_acc: 0.5666\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 16s 514ms/step - loss: 1.2173 - acc: 0.5934 - val_loss: 1.2671 - val_acc: 0.5644\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 1.2054 - acc: 0.5953 - val_loss: 1.2432 - val_acc: 0.5761\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 1.2036 - acc: 0.5991 - val_loss: 1.3174 - val_acc: 0.5431\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.1842 - acc: 0.6054 - val_loss: 1.2238 - val_acc: 0.5746\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 1.1726 - acc: 0.6101 - val_loss: 1.2486 - val_acc: 0.5699\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 1.1625 - acc: 0.6122 - val_loss: 1.2520 - val_acc: 0.5631\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 1.1594 - acc: 0.6128 - val_loss: 1.1981 - val_acc: 0.5939\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 1.1437 - acc: 0.6179 - val_loss: 1.1929 - val_acc: 0.5924\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 1.1337 - acc: 0.6210 - val_loss: 1.2087 - val_acc: 0.5826\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 1.1342 - acc: 0.6224 - val_loss: 1.2026 - val_acc: 0.5949\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 1.1282 - acc: 0.6260 - val_loss: 1.2024 - val_acc: 0.5911\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 1.1049 - acc: 0.6305 - val_loss: 1.1884 - val_acc: 0.5969\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 16s 504ms/step - loss: 1.1073 - acc: 0.6312 - val_loss: 1.1760 - val_acc: 0.5976\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 1.0932 - acc: 0.6348 - val_loss: 1.1830 - val_acc: 0.5921\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 1.0941 - acc: 0.6365 - val_loss: 1.1700 - val_acc: 0.6004\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.0901 - acc: 0.6393 - val_loss: 1.1861 - val_acc: 0.5994\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 1.0699 - acc: 0.6441 - val_loss: 1.1704 - val_acc: 0.6029\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 1.0666 - acc: 0.6478 - val_loss: 1.1596 - val_acc: 0.6044\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 1.0543 - acc: 0.6523 - val_loss: 1.1495 - val_acc: 0.6127\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.0538 - acc: 0.6508 - val_loss: 1.1873 - val_acc: 0.5954\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 1.0399 - acc: 0.6553 - val_loss: 1.2041 - val_acc: 0.5964\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.0374 - acc: 0.6509 - val_loss: 1.1393 - val_acc: 0.6069\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 1.0264 - acc: 0.6564 - val_loss: 1.1548 - val_acc: 0.6007\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 1.0198 - acc: 0.6625 - val_loss: 1.1314 - val_acc: 0.6124\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 1.0138 - acc: 0.6672 - val_loss: 1.1197 - val_acc: 0.6119\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 1.0029 - acc: 0.6698 - val_loss: 1.1326 - val_acc: 0.6059\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 17s 529ms/step - loss: 0.9966 - acc: 0.6674 - val_loss: 1.1006 - val_acc: 0.6309\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 16s 514ms/step - loss: 0.9921 - acc: 0.6706 - val_loss: 1.1194 - val_acc: 0.6179\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 0.9914 - acc: 0.6713 - val_loss: 1.1127 - val_acc: 0.6217\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.9744 - acc: 0.6791 - val_loss: 1.1141 - val_acc: 0.6207\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 0.9761 - acc: 0.6790 - val_loss: 1.1100 - val_acc: 0.6179\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.9675 - acc: 0.6786 - val_loss: 1.1008 - val_acc: 0.6292\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 0.9574 - acc: 0.6861 - val_loss: 1.1222 - val_acc: 0.6084\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.9520 - acc: 0.6855 - val_loss: 1.1037 - val_acc: 0.6249\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.9507 - acc: 0.6860 - val_loss: 1.1035 - val_acc: 0.6259\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.9419 - acc: 0.6890 - val_loss: 1.1205 - val_acc: 0.6197\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.9356 - acc: 0.6908 - val_loss: 1.1041 - val_acc: 0.6277\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.9361 - acc: 0.6906 - val_loss: 1.0797 - val_acc: 0.6332\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.9206 - acc: 0.7026 - val_loss: 1.0761 - val_acc: 0.6377\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.9164 - acc: 0.6996 - val_loss: 1.0780 - val_acc: 0.6357\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 16s 504ms/step - loss: 0.9119 - acc: 0.7007 - val_loss: 1.1491 - val_acc: 0.6139\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 16s 504ms/step - loss: 0.9095 - acc: 0.6992 - val_loss: 1.0844 - val_acc: 0.6384\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 16s 504ms/step - loss: 0.9051 - acc: 0.7008 - val_loss: 1.0871 - val_acc: 0.6322\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.8941 - acc: 0.7047 - val_loss: 1.0614 - val_acc: 0.6404\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.8881 - acc: 0.7057 - val_loss: 1.1052 - val_acc: 0.6242\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 0.8909 - acc: 0.7068 - val_loss: 1.0793 - val_acc: 0.6319\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 16s 516ms/step - loss: 0.8771 - acc: 0.7117 - val_loss: 1.0672 - val_acc: 0.6389\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.8665 - acc: 0.7175 - val_loss: 1.0617 - val_acc: 0.6344\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.8686 - acc: 0.7178 - val_loss: 1.0718 - val_acc: 0.6319\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.8607 - acc: 0.7191 - val_loss: 1.0807 - val_acc: 0.6282\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.8597 - acc: 0.7157 - val_loss: 1.0824 - val_acc: 0.6347\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.8506 - acc: 0.7226 - val_loss: 1.0744 - val_acc: 0.6407\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.8481 - acc: 0.7205 - val_loss: 1.0882 - val_acc: 0.6307\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 0.8354 - acc: 0.7255 - val_loss: 1.0476 - val_acc: 0.6447\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 0.8361 - acc: 0.7233 - val_loss: 1.1416 - val_acc: 0.6134\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 16s 504ms/step - loss: 0.8338 - acc: 0.7290 - val_loss: 1.0449 - val_acc: 0.6419\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.8202 - acc: 0.7302 - val_loss: 1.0850 - val_acc: 0.6354\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 16s 504ms/step - loss: 0.8202 - acc: 0.7327 - val_loss: 1.0467 - val_acc: 0.6464\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.8158 - acc: 0.7343 - val_loss: 1.0335 - val_acc: 0.6457\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.8059 - acc: 0.7381 - val_loss: 1.0974 - val_acc: 0.6382\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 0.8087 - acc: 0.7354 - val_loss: 1.0308 - val_acc: 0.6507\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.8021 - acc: 0.7344 - val_loss: 1.0544 - val_acc: 0.6414\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.7840 - acc: 0.7437 - val_loss: 1.0389 - val_acc: 0.6492\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.7922 - acc: 0.7403 - val_loss: 1.0261 - val_acc: 0.6597\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.7842 - acc: 0.7428 - val_loss: 1.0460 - val_acc: 0.6467\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.7825 - acc: 0.7438 - val_loss: 1.0786 - val_acc: 0.6474\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 0.7763 - acc: 0.7442 - val_loss: 1.0426 - val_acc: 0.6444\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 0.7698 - acc: 0.7463 - val_loss: 1.0505 - val_acc: 0.6467\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 16s 503ms/step - loss: 0.7653 - acc: 0.7468 - val_loss: 1.0428 - val_acc: 0.6467\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.7649 - acc: 0.7473 - val_loss: 1.0281 - val_acc: 0.6537\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.7597 - acc: 0.7542 - val_loss: 1.0638 - val_acc: 0.6409\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.7548 - acc: 0.7510 - val_loss: 1.0623 - val_acc: 0.6429\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.7525 - acc: 0.7538 - val_loss: 1.0183 - val_acc: 0.6634\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.7446 - acc: 0.7581 - val_loss: 1.0444 - val_acc: 0.6487\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.7384 - acc: 0.7613 - val_loss: 1.0225 - val_acc: 0.6587\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.7383 - acc: 0.7598 - val_loss: 1.0424 - val_acc: 0.6489\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 0.7254 - acc: 0.7657 - val_loss: 1.0217 - val_acc: 0.6617\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.7308 - acc: 0.7622 - val_loss: 1.0358 - val_acc: 0.6489\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.7200 - acc: 0.7667 - val_loss: 1.0005 - val_acc: 0.6617\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.7168 - acc: 0.7678 - val_loss: 1.0169 - val_acc: 0.6577\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.7125 - acc: 0.7661 - val_loss: 1.0284 - val_acc: 0.6572\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 16s 514ms/step - loss: 0.7011 - acc: 0.7725 - val_loss: 1.0393 - val_acc: 0.6469\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.7050 - acc: 0.7697 - val_loss: 1.0220 - val_acc: 0.6609\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.6973 - acc: 0.7705 - val_loss: 1.0483 - val_acc: 0.6479\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.7033 - acc: 0.7677 - val_loss: 1.0396 - val_acc: 0.6487\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 16s 516ms/step - loss: 0.6857 - acc: 0.7773 - val_loss: 1.0190 - val_acc: 0.6627\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 0.6835 - acc: 0.7743 - val_loss: 1.0408 - val_acc: 0.6539\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.6871 - acc: 0.7748 - val_loss: 1.1396 - val_acc: 0.6334\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.6725 - acc: 0.7800 - val_loss: 1.0166 - val_acc: 0.6574\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.6692 - acc: 0.7825 - val_loss: 1.0306 - val_acc: 0.6522\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.6696 - acc: 0.7820 - val_loss: 1.0048 - val_acc: 0.6632\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.6680 - acc: 0.7817 - val_loss: 1.0139 - val_acc: 0.6629\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.6589 - acc: 0.7888 - val_loss: 1.0402 - val_acc: 0.6457\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.6590 - acc: 0.7863 - val_loss: 1.0347 - val_acc: 0.6537\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.6474 - acc: 0.7884 - val_loss: 1.0099 - val_acc: 0.6599\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.6491 - acc: 0.7839 - val_loss: 1.0160 - val_acc: 0.6609\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.6479 - acc: 0.7884 - val_loss: 1.0013 - val_acc: 0.6654\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.6443 - acc: 0.7924 - val_loss: 1.0062 - val_acc: 0.6657\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.6417 - acc: 0.7885 - val_loss: 1.0100 - val_acc: 0.6599\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.6343 - acc: 0.7942 - val_loss: 1.0070 - val_acc: 0.6687\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.6272 - acc: 0.7949 - val_loss: 1.0376 - val_acc: 0.6599\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.6318 - acc: 0.7932 - val_loss: 1.0189 - val_acc: 0.6622\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.6179 - acc: 0.8022 - val_loss: 0.9998 - val_acc: 0.6667\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.6136 - acc: 0.7995 - val_loss: 1.0042 - val_acc: 0.6712\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.6217 - acc: 0.7967 - val_loss: 1.1251 - val_acc: 0.6389\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 0.6099 - acc: 0.8065 - val_loss: 1.0255 - val_acc: 0.6637\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 0.6122 - acc: 0.8029 - val_loss: 1.0297 - val_acc: 0.6594\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.6009 - acc: 0.8087 - val_loss: 1.0046 - val_acc: 0.6664\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.6003 - acc: 0.8043 - val_loss: 1.0220 - val_acc: 0.6587\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.5959 - acc: 0.8067 - val_loss: 0.9983 - val_acc: 0.6682\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 0.5985 - acc: 0.8075 - val_loss: 1.0278 - val_acc: 0.6614\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.5913 - acc: 0.8054 - val_loss: 1.0116 - val_acc: 0.6627\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.5829 - acc: 0.8117 - val_loss: 1.0154 - val_acc: 0.6644\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 0.5874 - acc: 0.8089 - val_loss: 1.0029 - val_acc: 0.6687\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 0.5804 - acc: 0.8142 - val_loss: 1.0075 - val_acc: 0.6677\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 0.5762 - acc: 0.8160 - val_loss: 1.0385 - val_acc: 0.6624\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 0.5615 - acc: 0.8170 - val_loss: 1.0322 - val_acc: 0.6579\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.5638 - acc: 0.8162 - val_loss: 1.0545 - val_acc: 0.6509\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 16s 512ms/step - loss: 0.5621 - acc: 0.8170 - val_loss: 1.1293 - val_acc: 0.6202\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 0.5602 - acc: 0.8166 - val_loss: 0.9882 - val_acc: 0.6764\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.5555 - acc: 0.8214 - val_loss: 1.0470 - val_acc: 0.6604\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.5611 - acc: 0.8177 - val_loss: 1.0365 - val_acc: 0.6497\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 16s 514ms/step - loss: 0.5494 - acc: 0.8214 - val_loss: 1.0294 - val_acc: 0.6632\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.5506 - acc: 0.8244 - val_loss: 0.9997 - val_acc: 0.6722\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.5463 - acc: 0.8237 - val_loss: 1.0721 - val_acc: 0.6477\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.5357 - acc: 0.8296 - val_loss: 1.0212 - val_acc: 0.6747\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.5422 - acc: 0.8258 - val_loss: 1.0228 - val_acc: 0.6642\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.5336 - acc: 0.8250 - val_loss: 1.0225 - val_acc: 0.6649\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.5346 - acc: 0.8257 - val_loss: 1.0133 - val_acc: 0.6732\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 0.5309 - acc: 0.8270 - val_loss: 1.0025 - val_acc: 0.6747\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 16s 510ms/step - loss: 0.5269 - acc: 0.8277 - val_loss: 1.0281 - val_acc: 0.6639\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.5203 - acc: 0.8317 - val_loss: 0.9975 - val_acc: 0.6752\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 16s 515ms/step - loss: 0.5193 - acc: 0.8324 - val_loss: 1.0033 - val_acc: 0.6749\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 16s 516ms/step - loss: 0.5178 - acc: 0.8352 - val_loss: 1.0550 - val_acc: 0.6582\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 16s 515ms/step - loss: 0.5218 - acc: 0.8309 - val_loss: 1.0463 - val_acc: 0.6637\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.5057 - acc: 0.8369 - val_loss: 1.0338 - val_acc: 0.6639\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.5045 - acc: 0.8389 - val_loss: 1.1120 - val_acc: 0.6552\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.5112 - acc: 0.8369 - val_loss: 1.0291 - val_acc: 0.6742\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.5030 - acc: 0.8385 - val_loss: 1.0092 - val_acc: 0.6797\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 16s 503ms/step - loss: 0.5005 - acc: 0.8391 - val_loss: 1.0569 - val_acc: 0.6667\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 0.4985 - acc: 0.8404 - val_loss: 1.0091 - val_acc: 0.6772\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 16s 505ms/step - loss: 0.4964 - acc: 0.8394 - val_loss: 1.0739 - val_acc: 0.6537\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 16s 506ms/step - loss: 0.4817 - acc: 0.8462 - val_loss: 1.0143 - val_acc: 0.6692\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.4893 - acc: 0.8426 - val_loss: 1.1273 - val_acc: 0.6472\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 16s 513ms/step - loss: 0.4865 - acc: 0.8453 - val_loss: 1.0042 - val_acc: 0.6797\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 0.4842 - acc: 0.8432 - val_loss: 1.1840 - val_acc: 0.6322\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 16s 511ms/step - loss: 0.4820 - acc: 0.8469 - val_loss: 1.0073 - val_acc: 0.6777\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 16s 508ms/step - loss: 0.4779 - acc: 0.8477 - val_loss: 1.0483 - val_acc: 0.6662\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.4736 - acc: 0.8439 - val_loss: 1.0039 - val_acc: 0.6794\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 16s 507ms/step - loss: 0.4742 - acc: 0.8464 - val_loss: 0.9990 - val_acc: 0.6812\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.9882 - acc: 0.6764\n",
            "[0.9882373213768005, 0.6764190793037415]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqBtExMZ5kEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "511588fc-2a0e-4991-884f-db854b7550a6"
      },
      "source": [
        "print(model_wedge.evaluate(x_val, y_val))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 2s 17ms/step - loss: 0.9882 - acc: 0.6764\n",
            "[0.9882373213768005, 0.6764190793037415]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf7NeLv86Paf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}