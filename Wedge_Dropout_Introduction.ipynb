{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wedge Dropout Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1m2kDyocl9I"
      },
      "source": [
        "# Wedge Dropout\n",
        "## Abstract\n",
        "[ need logo of wedge ]\n",
        "\n",
        "Wedge Dropout is one of a recent wave of Dropout algorithms which are customized for Convolution Neural Networks (CNNs). Wedge Dropout has a very simple goal: it attempts to de-correlate feature maps in a CNN by examining pairs of feature maps and zeroing them both out if they are \"too similar\". At heart, the idea is that a CNN has a set of engines for creating feature maps from an image, and these engines become too similar. Wedge Dropout discovers that two of these engines are too close together, and \"drives a wedge\" between them to push them apart. Wedge Dropout is suitable for Convolutional Networks using 1D, 2D, 3D, and LSTM2D layers. It is best used at the end of a pipeline of CNN layers, as it operates by critiquing the values created by the CNN pipeline. Wedge Dropout operates by critiquing the values created by the CNN pipeline, and is best used at the end of a pipeline of CNN layers.\n",
        "[ need logo of mallet driving wedge between two lines that are almost touching]\n",
        "Preliminary testing has shown that Wedge Dropout can give a very slight improvement to model accuracy, generally 0.05% to 0.1%, in many different 1D, 2D and 3D CNN-based models. There is usually no hyperparameter tuning required to achieve this improvement. Wedge Dropout has no run-time overhead; however, it can increase training time by up to 10%, depending on how many feature maps the model uses in its final stage.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IEkdVQQBsym"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJtfWV1YpAT_"
      },
      "source": [
        "## Introduction\n",
        "Wedge Dropout is a Dropout algorithm tuned for convolutional networks. The Wedge Dropout algorithm is a variation of Spatial Dropout.\n",
        "Where Dropout zeros out individual values in a feature map, Spatial Dropout randomly zeros out entire feature maps. Since data within a feature map is strongly correlated, simple Dropout is not effective because the remaining data is still strongly correlated. \n",
        "\n",
        "(It tends to have the most useful at improving a neural network when used after the first convolutional layer.)t\n",
        "[ Assumptions: understand neural networks, CNNs, and the role Dropout plays ]\n",
        " A CNN works by generating overlapping pyramids based on an input image. These pyramids are called \"feature maps\". Given a picture of a cat, one feature map outlines the head, another the eyes, a third the ears. Collectively they describe aspects of the cat. Feature maps should be independent- if they are correlated, or \"too similar\", the CNN describes fewer unique aspects than it could.\n",
        "\n",
        "The Wedge Dropout algorithm works by analyzing the final output of a convolutional neural network (CNN). Where Spatial Dropout zeroes out randomly chosen feature maps, Wedge Dropout analyzes randomly chosen pairs of feature maps and drops both when they are \"too similar\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Y1vWhAEyhX"
      },
      "source": [
        "## Similar Work\n",
        "Wedge Dropout was directly inspired by CamDrop. CamDrop analyzes all of the feature maps in a set and chooses a rectangle that will be zeroed out in some of the feature maps. Other similar algorithms are DropBlock, which randomly zeroes out rectangles in feature maps. In a sense, Wedge Dropout is to Spatial Dropout as CamDrop is to DropBlock: it replaces randomness with analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQWWMkx5Ff36"
      },
      "source": [
        "## Algorithms\n",
        "There are two algorithms implemented for Wedge Dropout's analysis phase. \n",
        "### Direct Comparison \n",
        "The direct comparison algorithm isolates the cells in each feature map which are above its mean value, and then compares individual values across both feature maps cell by cell in a simple Boolean AND operation. This counts the number of cells in both feature maps that tend to find the same feature. If this count is above a certain percentage of the total number of cells in the feature map, both feature maps are zeroed out.\n",
        "### Normalize And Multiply\n",
        "This algorithm normalizes both feature maps to a range from 0.0 to 1.0, multiplies the two feature maps cellwise (Hadamard matrix multiplication), and counts the number of values above the mean. Again, if more than a certain percentage are above the mean (0.5), both feature maps are zeroed out.\n",
        "\n",
        "Normalize and Multiply's Hadamard multiplication has the effect of exaggerating the pairwise comparison, versus Direct Comparison. It is not clear that either algorithm is better.\n",
        "\n",
        "Following are implementations of each algorithm in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9f5wLiQck1o"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvmrcvQ8uvLw",
        "outputId": "902e0199-76d7-4cb6-f35d-b4d55343a16d"
      },
      "source": [
        "def wedge_direct_comparison(img1, img2):\n",
        "    # This is exactly what a GlobalAveragePooling2D layer does\n",
        "    mean1 = np.mean(img1)\n",
        "    mean2 = np.mean(img2)\n",
        "\n",
        "    visible1 = img1 > mean1\n",
        "    visible2 = img2 > mean2\n",
        "\n",
        "    correlated = visible1 == visible2\n",
        "\n",
        "    percentage = sum(correlated.flatten()) / len(img1.flatten())\n",
        "\n",
        "    return correlated, percentage\n",
        "\n",
        "\n",
        "def wedge_normalize(img1, img2):\n",
        "    # normalize values in both images now range from 0 to 1 \n",
        "    # the mean of all possible images is 0.5\n",
        "    # somehow, there is no numpy function for this\n",
        "    norm1 = (img1 - np.min(img1)) / (np.max(img1) - np.min(img1))\n",
        "    norm2 = (img2 - np.min(img2)) / (np.max(img2) - np.min(img2))\n",
        "\n",
        "    # create a new matrix by Hadamard multiplication, elementwise\n",
        "    # this exaggerates large values in both feature maps\n",
        "    compare = norm1[:, :] * norm2[:, :]\n",
        "\n",
        "    # Find the number of values greater than average\n",
        "    correlated = sum(compare > 0.5)\n",
        "\n",
        "    # Find the percentage of values greater than average\n",
        "    percentage = sum(correlated.flatten()) / len(img1.flatten())\n",
        "\n",
        "    return compare > 0.5, percentage\n",
        "\n",
        "img1 = np.asarray([[1,2],[3,4]])\n",
        "img2 = np.asarray([[5,6],[7,3]])\n",
        "\n",
        "print('Feature Map #1')\n",
        "print(img1)\n",
        "print('Feature Map #2')\n",
        "print(img2)\n",
        "print()\n",
        "\n",
        "correlation1, percentage1 = wedge_direct_comparison(img1, img2)\n",
        "print('Correlation (count > mean): ')\n",
        "print(correlation1)\n",
        "print('  similarity score:', percentage1)\n",
        "correlation2, percentage2 = wedge_normalize(img1, img2)\n",
        "print('Correlation (normalized and multiplied): ')\n",
        "print(correlation2)\n",
        "print('  similarity score:', percentage2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature Map #1\n",
            "[[1 2]\n",
            " [3 4]]\n",
            "Feature Map #2\n",
            "[[5 6]\n",
            " [7 3]]\n",
            "\n",
            "Correlation (count > mean): \n",
            "[[ True False]\n",
            " [ True False]]\n",
            "  similarity score: 0.5\n",
            "Correlation (normalized and multiplied): \n",
            "[[False False]\n",
            " [ True False]]\n",
            "  similarity score: 0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OsHP8gS4daO"
      },
      "source": [
        "We can see that the two algorithms provide a different interpretation of 'correlated'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7oNE8xevL8l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taWg9w49KPq2"
      },
      "source": [
        "# Usage\n",
        "Wedge Dropout operates by critiquing the values created by the CNN pipeline, and is best used at the end of a pipeline of CNN layers. Like Spatial Dropout and other CNN-specific Dropout algorithms, do not place a Batch Normalization layer after a Wedge Dropout layer. Wedge Dropout works well after a Batch Normalization layer. All successful tests have placed the Wedge Dropout layer right before the final summarization layer, usually a Dense or GlobalAveragePooling layer.\n",
        "\n",
        "[ Lift a standard CNN diagram and poke in a Wedge Dropout layer right before the final Dense/GlobalAveragePooling layer. ]\n",
        "\n",
        "Wedge Dropout only has one hyperparameter, the similarity coefficient. Preliminary testing on many different 1D, 2D and 3D CNN networks has shown that one value is optimal for almost all applications: 0.5 for the Direct Comparison algorithm, and 0.65 for the Normalize and Multiply algorithm.\n",
        "\n",
        "# Batch-wise Operation\n",
        "It has proven very powerful to apply Wedge Dropout to all of the feature maps for a pair of random indexes, and then do a simple voting algorithm on the results. For example, if the batch size is 32, then if 17 pairs of feature maps are \"too similar\", then all 32 feature maps are zeroed out. If only 15 pairs are too similar, none of the feature maps are zeroed out.\n",
        "\n",
        "When operating per sample, Wedge Dropout critiques a pair of feature maps. In batch-wise operation, Wedge Dropout critiques the engine that created the feature maps. \n",
        "\n",
        "Batch Normalization is a proven method of improving a CNN model, and is used in most reference architectures except image generators. Batch Normalization's performance improves as the batch size increases. Wedge Dropout's performance also increases as batch size increases, so it is a good match for existing CNN architectures.\n",
        "Wedge Dropout works well after a Batch Normalization layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwlZGeKuPa45"
      },
      "source": [
        "# Concluding Remarks\n",
        "The Wedge Dropout algorithm can add a noticeable increase in tensor graph compilation time, and training time per epoch. For example, the final phase of EfficientNet \"Zero\" generates 1024 feature maps, and then applies GlobalAveragePooling to those. Wedge Dropout was most effective when inserted between the final convolution layer and the GlobalAveralPooling layer. Adding Wedge Dropout increased the tensor graph compilation phase by 20%, and added 10%-15% to the running time for each epoch. Wedge Dropout is not active during prediction, and does not contribute any values that need to be loaded for a model.\n",
        "\n",
        "## Utility\n",
        "Wedge Dropout is not needed for any image architecture. However, given the lack of complex hyperparameters and simplicity of application, it will probably improve most production uses of CNNs.\n",
        "It has been tested with many example networks in the Keras documentation, and worked in all Conv1D, Conv2D and Conv3D-based applications. It did not work well with LSTM2D, possibly due to muddy causality in feature map creation. \n",
        "\n",
        "## Further Investigation\n",
        "\"Slice Dropout\" [ SliceDropout ], a variant of Spatial Dropout, zeroes out only one half of a feature map instead of the entire feature map. A more complex version of Wedge Dropout's feature map comparison could decide that all of the offending cells are concentrated on one side of the feature map, or even just one quadrant. It would choose to zero out only that area.\n",
        "\n",
        "It is possible that zeroing out values is not the only way to affect training. There may be ways to do a random fill which do not disrupt the operation of Batch Normalization.\n",
        "\n",
        "### Feature Maps and Attention Heads\n",
        "The multi-head attention [ cite ] architecture creates several \"answer\" vectors of the same size, and combines them with addition. This set of vectors looks suspiciously like a set of feature maps: the information within the vector is strongly correlated, but information across vectors is not correlated. In fact, if the information is correlated across attention heads, the attention heads are learning the same answer set. It is possible that Wedge Dropout, or even Spatial Dropout, will improve the function of a multi-head attention model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdHlFun6Layc"
      },
      "source": [
        "# Citations\n",
        "[ CamDrop ]\n",
        "\n",
        "[ DropBlock ]\n",
        "\n",
        "[ SpatialDropout paper ]\n",
        "\n",
        "[ SpatialDropout Keras man page ]"
      ]
    }
  ]
}